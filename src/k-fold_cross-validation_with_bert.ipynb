{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5649469-7d13-4df9-ab8c-b3b47ad89d50",
   "metadata": {},
   "source": [
    "### `K-Fold Cross-validation with BERT`\n",
    "\n",
    "The text is cleaned and should be tokenized in a way that the embeddings can be extracted for K-Fold cross-validation with the `BERT Sequence Classification` model.\n",
    "\n",
    "First, let's install the pytorch interface for Bert by Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05c1471a-979c-431e-ab54-cf8752d3af74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pytorch-pretrained-bert pytorch-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c39e3c0-c189-4429-a6a7-fcafb5a7a46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "489c39c0-9952-4650-bba1-acce760a7eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22830, 9)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets = pd.read_csv('../data/preprocessed_tweets.csv')\n",
    "df_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c8b0e50-7ec3-4015-9fb1-c81cb3a68193",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vader_sentiment_label</th>\n",
       "      <th>vader_score</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_length</th>\n",
       "      <th>url_link</th>\n",
       "      <th>pos_emoji</th>\n",
       "      <th>neg_emoji</th>\n",
       "      <th>profanity_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1851</th>\n",
       "      <td>1</td>\n",
       "      <td>0.4101</td>\n",
       "      <td>finna go gma hou escap depress make happi</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15938</th>\n",
       "      <td>1</td>\n",
       "      <td>0.5994</td>\n",
       "      <td>peopl anxieti depress flaunt around like medal...</td>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10536</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>relat newfoundland bankrupt depress effecti ha...</td>\n",
       "      <td>259</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5517</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.7606</td>\n",
       "      <td>depress hit got ta remind bitch like aye wtf d...</td>\n",
       "      <td>104</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12501</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.7236</td>\n",
       "      <td>watch coupl episod st season battl depress cho...</td>\n",
       "      <td>243</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14757</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.6771</td>\n",
       "      <td>must watch everyon especi one suffer mental di...</td>\n",
       "      <td>139</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2412</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.5719</td>\n",
       "      <td>ever thought area might contain cure depress</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12181</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.1531</td>\n",
       "      <td>friend plea pray mi wouldst depress pain deal ...</td>\n",
       "      <td>147</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21198</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.2960</td>\n",
       "      <td>depress long therapi gap like</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19318</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9779</td>\n",
       "      <td>hi want say actual love ygt sm help thru depre...</td>\n",
       "      <td>234</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       vader_sentiment_label  vader_score  \\\n",
       "1851                       1       0.4101   \n",
       "15938                      1       0.5994   \n",
       "10536                      0       0.0258   \n",
       "5517                       0      -0.7606   \n",
       "12501                      0      -0.7236   \n",
       "14757                      0      -0.6771   \n",
       "2412                       0      -0.5719   \n",
       "12181                      0      -0.1531   \n",
       "21198                      0      -0.2960   \n",
       "19318                      1       0.9779   \n",
       "\n",
       "                                                   tweet  tweet_length  \\\n",
       "1851           finna go gma hou escap depress make happi            72   \n",
       "15938  peopl anxieti depress flaunt around like medal...           107   \n",
       "10536  relat newfoundland bankrupt depress effecti ha...           259   \n",
       "5517   depress hit got ta remind bitch like aye wtf d...           104   \n",
       "12501  watch coupl episod st season battl depress cho...           243   \n",
       "14757  must watch everyon especi one suffer mental di...           139   \n",
       "2412        ever thought area might contain cure depress            76   \n",
       "12181  friend plea pray mi wouldst depress pain deal ...           147   \n",
       "21198                      depress long therapi gap like            47   \n",
       "19318  hi want say actual love ygt sm help thru depre...           234   \n",
       "\n",
       "       url_link  pos_emoji  neg_emoji  profanity_word  \n",
       "1851          0          0          0               0  \n",
       "15938         0          0          0               0  \n",
       "10536         0          0          0               0  \n",
       "5517          0          0          0               2  \n",
       "12501         0          0          0               0  \n",
       "14757         1          0          0               0  \n",
       "2412          0          0          0               0  \n",
       "12181         0          0          0               0  \n",
       "21198         0          0          0               0  \n",
       "19318         0          0          0               0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets = df_tweets.drop(['Unnamed: 0'], axis=1)\n",
    "df_tweets.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a7c58f-32f8-4ff2-94ea-68397e42e98d",
   "metadata": {},
   "source": [
    "### Inputs\n",
    "**BERT** requires specifically formatted inputs. For each tokenized input sentence, we need to create:\n",
    "\n",
    "- **input ids**: a sequence of integers identifying each input token to its index number in the BERT tokenizer vocabulary\n",
    "- **segment mask**: (optional) a sequence of 1s and 0s used to identify whether the input is one sentence or two sentences long. For one sentence inputs, this is simply a sequence of 0s. For two sentence inputs, there is a 0 for each token of the first sentence, followed by a 1 for each token of the second sentence. Will not be used in this project.\n",
    "- **attention mask**: (optional) a sequence of 1s and 0s, with 1s for all input tokens and 0s for all padding tokens (we'll detail this in the next paragraph)\n",
    "- **labels**: a single value of 1 or 0. In our task 1 means \"grammatical\" and 0 means \"ungrammatical\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0e8d0f71-7a69-4328-9f73-3c4b2da86c44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m BertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:918\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    902\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \n\u001b[1;32m    904\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:918\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    902\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \n\u001b[1;32m    904\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27756e18-5219-4c2c-a810-48c1c036419c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1650 Ti'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6723eb47-a62a-4d1a-9dec-7c6abbdd7163",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_FOLDS = 10\n",
    "MAX_LEN = 128\n",
    "PADDING = 'post'\n",
    "TRUNCATING = 'post'\n",
    "DTYPE = 'long'\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3a89319-37d9-43cf-80d9-4026a230c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns tokens of the tweet, and tensors of the tokens and segment ids\n",
    "def tokenization(tweets, maxlen=MAX_LEN, dtype=DTYPE, truncating=TRUNCATING, padding=PADDING, tokenizer=tokenizer):\n",
    "    tweets = [\"[CLS] \" + tweet + \" [SEP]\" for tweet in tweets]\n",
    "    tokenized_texts = [tokenizer.tokenize(tweet) for tweet in tweets]\n",
    "\n",
    "    # Map the token strings to their vocabulary indeces.\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(text) for text in tokenized_texts]\n",
    "\n",
    "    # Pad our input tokens\n",
    "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=dtype, truncating=truncating, padding=padding)\n",
    "\n",
    "    # Create attention masks\n",
    "    attention_masks = []\n",
    "\n",
    "    for seq in input_ids:\n",
    "      seq_mask = [float(i>0) for i in seq]\n",
    "      attention_masks.append(seq_mask)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    # tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    # segments_tensor = torch.tensor([segments_ids])\n",
    "\n",
    "    return tokenized_texts, input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87834b96-adf6-46f5-bdf2-9a309ec5a317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 2e-5\n",
    "EPOCHS = 3\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = BertAdam(optimizer_grouped_parameters, lr=LEARNING_RATE, warmup=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b3de3b4d-a5b2-451f-aa8e-21ef6c396454",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df_tweets.tweet.values\n",
    "labels = df_tweets.vader_sentiment_label.values\n",
    "\n",
    "_, input_ids, attention_masks = tokenization(tweets) \n",
    "dataset = {'input_ids': input_ids, 'attention_masks': attention_masks, 'labels': torch.tensor(labels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f8bb2a-b8e9-4aa9-9299-d9671da910f3",
   "metadata": {},
   "source": [
    "### K-fold Crossvalidation on BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ce704459-5088-4667-af1c-bb54fa12a8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvalidation(df, dataset, device, optimizer, epochs=EPOCHS, k_folds=K_FOLDS, batch_size=BATCH_SIZE):\n",
    "\n",
    "    # Define k-fold cross-validation\n",
    "    skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize lists to store accuracies for each fold\n",
    "    fold_accuracies = []\n",
    "    \n",
    "    # Perform k-fold cross-validation\n",
    "    for fold, (train_indices, val_indices) in enumerate(skf.split(df['tweet'], df['vader_sentiment_label'])):\n",
    "        print(f\"Training fold: {fold+1}/{k_folds}\")\n",
    "\n",
    "        # Split dataset into train and validation sets for the current fold\n",
    "        train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "        val_dataset =  torch.utils.data.Subset(dataset, val_indices)\n",
    "\n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Training loop\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "        for _ in trange(epochs, desc=\"Epoch\"):\n",
    "            for batch in train_loader:\n",
    "                # clear out the gradients (by default they accumulate)\n",
    "                optimizer.zero_grad()\n",
    "                # add batch to GPU\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                # unpack the inputs from dataloader\n",
    "                b_input_ids, b_input_mask, b_labels = batch\n",
    "                # forward pass\n",
    "                outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "                loss = outputs.loss\n",
    "                # backward pass\n",
    "                loss.backward()\n",
    "                # update parameters and take a step using the computed gradient\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        # Put model on evaluation mode to evaluate loss on the validation set\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables\n",
    "        val_predictions, val_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            # Evaluate the data for one epoch\n",
    "            for batch in val_loader:\n",
    "                # add batch to GPU\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                # unpack the inputs from dataloader\n",
    "                b_input_ids, b_input_mask, b_labels = batch\n",
    "                logits = model(input_ids, attention_mask=attention_mask)\n",
    "                _, predicted_labels = torch.max(outputs.logits, dim=1)\n",
    "                val_predictions.extend(predicted_labels.tolist())\n",
    "                val_labels.extend(labels.tolist())\n",
    "\n",
    "        fold_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "        fold_accuracies.append(fold_accuracy)\n",
    "        print(f\"Accuracy for Fold {fold+1}: {fold_accuracy}\")\n",
    "\n",
    "    # Calculate average accuracy across all folds\n",
    "    average_accuracy = sum(fold_accuracies) / len(fold_accuracies)\n",
    "    print(f\"Average Accuracy: {average_accuracy}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647a74d9-c95a-494b-8c65-95a4bf273b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
