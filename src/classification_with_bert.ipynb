{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1b4da41-23c8-4f0b-a998-2f13d22b1b94",
   "metadata": {},
   "source": [
    "### `Tokenization and Embedding Extraction with BERT`\n",
    "\n",
    "The text is cleaned and should be tokenized in a way thay the embeddings can be extracted for later training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c861f725-9d21-4b77-aa82-9bd50a2c4778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "364ec295-33ba-45a9-a371-ba3a62adf5d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22830, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets = pd.read_csv('../data/preprocessed_tweets.csv')\n",
    "df_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e52e3dc2-d89d-46ee-82f5-90cee6003da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vader_sentiment_label</th>\n",
       "      <th>vader_score</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_length</th>\n",
       "      <th>url_link</th>\n",
       "      <th>pos_emoji</th>\n",
       "      <th>neg_emoji</th>\n",
       "      <th>profanity_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22027</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.9442</td>\n",
       "      <td>one summati test away depress mamiii</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15897</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.5719</td>\n",
       "      <td>call depress</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15679</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.8547</td>\n",
       "      <td>achiev none obama took economi spin toward dep...</td>\n",
       "      <td>280</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18979</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.9828</td>\n",
       "      <td>tw suici would sh fuck two littl girl took li ...</td>\n",
       "      <td>277</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12368</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.9118</td>\n",
       "      <td>damn bro depress shit wack af like tf u stop s...</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17135</th>\n",
       "      <td>1</td>\n",
       "      <td>0.1513</td>\n",
       "      <td>increa screen time link greater risk depress a...</td>\n",
       "      <td>252</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6163</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.2960</td>\n",
       "      <td>season depress look like one</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2336</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0314</td>\n",
       "      <td>like pl know queen anxieti felt almost everi f...</td>\n",
       "      <td>181</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3509</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.7269</td>\n",
       "      <td>depress obsess xxxtentacion</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21859</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>apolog lack activ late combin busi travel vaca...</td>\n",
       "      <td>262</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       vader_sentiment_label  vader_score  \\\n",
       "22027                      0      -0.9442   \n",
       "15897                      0      -0.5719   \n",
       "15679                      0      -0.8547   \n",
       "18979                      0      -0.9828   \n",
       "12368                      0      -0.9118   \n",
       "17135                      1       0.1513   \n",
       "6163                       0      -0.2960   \n",
       "2336                       0       0.0314   \n",
       "3509                       0      -0.7269   \n",
       "21859                      0      -0.0000   \n",
       "\n",
       "                                                   tweet  tweet_length  \\\n",
       "22027               one summati test away depress mamiii            51   \n",
       "15897                                       call depress            34   \n",
       "15679  achiev none obama took economi spin toward dep...           280   \n",
       "18979  tw suici would sh fuck two littl girl took li ...           277   \n",
       "12368  damn bro depress shit wack af like tf u stop s...           100   \n",
       "17135  increa screen time link greater risk depress a...           252   \n",
       "6163                        season depress look like one            42   \n",
       "2336   like pl know queen anxieti felt almost everi f...           181   \n",
       "3509                         depress obsess xxxtentacion            40   \n",
       "21859  apolog lack activ late combin busi travel vaca...           262   \n",
       "\n",
       "       url_link  pos_emoji  neg_emoji  profanity_word  \n",
       "22027         0          0          0               0  \n",
       "15897         0          0          0               0  \n",
       "15679         0          0          0               0  \n",
       "18979         1          0          0               1  \n",
       "12368         0          0          0               2  \n",
       "17135         1          0          0               0  \n",
       "6163          1          0          0               0  \n",
       "2336          0          0          0               0  \n",
       "3509          1          0          0               0  \n",
       "21859         0          0          0               0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets = df_tweets.drop(['Unnamed: 0'], axis=1)\n",
    "df_tweets.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc45edb-ba18-472c-a1f2-9d50ce64d0ff",
   "metadata": {},
   "source": [
    "### Inputs\n",
    "**BERT** requires specifically formatted inputs. For each tokenized input sentence, we need to create:\n",
    "\n",
    "- **input ids**: a sequence of integers identifying each input token to its index number in the BERT tokenizer vocabulary\n",
    "- **segment mask**: (optional) a sequence of 1s and 0s used to identify whether the input is one sentence or two sentences long. For one sentence inputs, this is simply a sequence of 0s. For two sentence inputs, there is a 0 for each token of the first sentence, followed by a 1 for each token of the second sentence. Will not be used in this project.\n",
    "- **attention mask**: (optional) a sequence of 1s and 0s, with 1s for all input tokens and 0s for all padding tokens (we'll detail this in the next paragraph)\n",
    "- **labels**: a single value of 1 or 0. In our task 1 means \"grammatical\" and 0 means \"ungrammatical\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10b0ebd8-9985-41dd-874a-6f88c5dfdf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "PADDING = 'post'\n",
    "TRUNCATING = 'post'\n",
    "TRAINING_SPLIT = .75\n",
    "DTYPE = 'long'\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f98292-4116-4cf9-a6a4-d859672fcd93",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31ea3bcc-a2b5-4451-a69a-a9988426035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Returns tokens of the tweet, and tensors of the tokens and segment ids\n",
    "def tokenization(tweets, maxlen=MAX_LEN, dtype=DTYPE, truncating=TRUNCATING, padding=PADDING, tokenizer=tokenizer):\n",
    "    tweets = [\"[CLS] \" + tweet + \" [SEP]\" for tweet in tweets]\n",
    "    tokenized_texts = [tokenizer.tokenize(tweet) for tweet in tweets]\n",
    "\n",
    "    # Map the token strings to their vocabulary indeces.\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(text) for text in tokenized_texts]\n",
    "    \n",
    "    # Pad our input tokens\n",
    "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=dtype, truncating=truncating, padding=padding)\n",
    "    \n",
    "    # Create attention masks\n",
    "    attention_masks = []\n",
    "\n",
    "    for seq in input_ids:\n",
    "      seq_mask = [float(i>0) for i in seq]\n",
    "      attention_masks.append(seq_mask)\n",
    "    \n",
    "    # Convert inputs to PyTorch tensors\n",
    "    # tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    # segments_tensor = torch.tensor([segments_ids])\n",
    "\n",
    "    return tokenized_texts, input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a96b2bb0-9e78-4464-8837-db9da2b925fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wow dad yesterday take stupi would depress drug anymor though absolut worst thing never need great famili supporti mom sister stanc similar way'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create tweets and labels lists\n",
    "tweets = df_tweets.tweet.values\n",
    "labels = df_tweets.vader_sentiment_label.values\n",
    "tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9813d163-df7c-41a1-8794-f17e4331907e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[CLS]',\n",
       "  'wow',\n",
       "  'dad',\n",
       "  'yesterday',\n",
       "  'take',\n",
       "  'stu',\n",
       "  '##pi',\n",
       "  'would',\n",
       "  'de',\n",
       "  '##press',\n",
       "  'drug',\n",
       "  'any',\n",
       "  '##mo',\n",
       "  '##r',\n",
       "  'though',\n",
       "  'abs',\n",
       "  '##ol',\n",
       "  '##ut',\n",
       "  'worst',\n",
       "  'thing',\n",
       "  'never',\n",
       "  'need',\n",
       "  'great',\n",
       "  'fa',\n",
       "  '##mi',\n",
       "  '##li',\n",
       "  'support',\n",
       "  '##i',\n",
       "  'mom',\n",
       "  'sister',\n",
       "  'stan',\n",
       "  '##c',\n",
       "  'similar',\n",
       "  'way',\n",
       "  '[SEP]'],\n",
       " array([  101, 10166,  3611,  7483,  2202, 24646,  8197,  2052,  2139,\n",
       "        20110,  4319,  2151,  5302,  2099,  2295, 14689,  4747,  4904,\n",
       "         5409,  2518,  2196,  2342,  2307,  6904,  4328,  3669,  2490,\n",
       "         2072,  3566,  2905,  9761,  2278,  2714,  2126,   102,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0]),\n",
       " [1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_texts, input_ids, attention_masks = tokenization(tweets)\n",
    "tokenized_texts[0], input_ids[0], attention_masks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05a847af-d266-49a8-8900-cd9daaa20360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(input_ids, attention_masks, labels, training_split=TRAINING_SPLIT, batch_size=BATCH_SIZE):\n",
    "    # Use train_test_split to split our data into train and validation sets for training\n",
    "    train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels,\n",
    "                                                                                        random_state=2018, train_size=training_split)\n",
    "    train_masks, validation_masks, train_masks, validation_masks = train_test_split(attention_masks, input_ids,\n",
    "                                                                                        random_state=2018, train_size=training_split)\n",
    "\n",
    "    # Convert all of our data into torch tensors, the required datatype for our model\n",
    "    train_inputs = torch.tensor(train_inputs)\n",
    "    validation_inputs = torch.tensor(validation_inputs)\n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    validation_labels = torch.tensor(validation_labels)\n",
    "    train_masks = torch.tensor(train_masks)\n",
    "    validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "    # Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "    # with an iterator the entire dataset does not need to be loaded into memory\n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    \n",
    "    validation_data = TensorDataset(validation_inputs, validation_masks, validation_masks)\n",
    "    validation_sampler = SequentialSampler(validation_data)\n",
    "    validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "    \n",
    "    return train_dataloader, validation_dataloader                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ece19373-0f5c-4f91-9622-e4feb3299d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, validation_dataloader = train_val_split(input_ids, attention_masks, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2f15f8-d552-464c-8f43-ed5c87d2beb5",
   "metadata": {},
   "source": [
    "### Loading and Fine-Tuning BERT\n",
    "Now that our input data is properly formatted, it's time to fine tune the BERT model. \n",
    "\n",
    "For this task, we first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task. Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task.  \n",
    "\n",
    "Because the pre-trained BERT layers already encode a lot of information about the language, training the classifier is relatively inexpensive. Rather than training every layer in a large model from scratch, it's as if we have already trained the bottom layers 95% of where they need to be, and only really need to train the top layer, with a bit of tweaking going on in the lower levels to accomodate our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "124be261-37ba-4228-8dcb-6d5a57e3f730",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 407873900/407873900 [05:03<00:00, 1345559.75B/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 3.81 GiB of which 13.12 MiB is free. Including non-PyTorch memory, this process has 3.79 GiB memory in use. Of the allocated memory 3.67 GiB is allocated by PyTorch, and 56.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m BertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:918\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    902\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \n\u001b[1;32m    904\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 810 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:918\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    902\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \n\u001b[1;32m    904\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 3.81 GiB of which 13.12 MiB is free. Including non-PyTorch memory, this process has 3.79 GiB memory in use. Of the allocated memory 3.67 GiB is allocated by PyTorch, and 56.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2b0d68-b29f-4826-887a-aba35a4e126d",
   "metadata": {},
   "source": [
    "In order for torch to use the GPU, we need to identify and specify the GPU as \n",
    "the device. Later, in our training loop, we will load data onto the device. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "038cd977-f9e4-4ab8-9b36-64e374a3198e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1650 Ti'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62191801-c354-4452-b611-fb7ed6891937",
   "metadata": {},
   "source": [
    "Below is our training loop. There's a lot going on, but fundamentally for each pass in our loop we have a trianing phase and a validation phase. At each pass we need to:\n",
    "\n",
    "Training loop:\n",
    "- Tell the model to compute gradients by setting the model in train mode\n",
    "- Unpack our data inputs and labels\n",
    "- Load data onto the GPU for acceleration\n",
    "- Clear out the gradients calculated in the previous pass. In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out\n",
    "- Forward pass (feed input data through the network)\n",
    "- Backward pass (backpropagation)\n",
    "- Tell the network to update parameters with optimizer.step()\n",
    "- Track variables for monitoring progress\n",
    "\n",
    "Evalution loop:\n",
    "- Tell the model not to compute gradients by setting the model in evaluation mode\n",
    "- Unpack our data inputs and labels\n",
    "- Load data onto the GPU for acceleration\n",
    "- Forward pass (feed input data through the network)\n",
    "- Compute loss on our validation data and track variables for monitoring progress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dff34f4d-2225-453d-9a30-9971639cecb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 2e-5 \n",
    "EPOCHS = 4\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = BertAdam(optimizer_grouped_parameters, lr=LEARNING_RATE, warmup=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c7d586b-c9e5-499f-81af-1c607829a95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd04a52f-d6d9-40ae-9596-05167b274abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validation(train_dataloader, validation_dataloader, model, device, optimizer, epochs=EPOCHS):\n",
    "    # Store our loss and accuracy for plotting\n",
    "    train_loss_set = []\n",
    "    \n",
    "    for _ in trange(epochs, desc=\"Epoch\"):\n",
    "\n",
    "        # Training\n",
    "        # set our model to training mode (as opposed to evaluation mode)\n",
    "        model.train()\n",
    "\n",
    "        # tracking variables\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "        # train the data for one epoch\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # add batch to GPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            # unpack the inputs from dataloader\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            # clear out the gradients (by default they accumulate)\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            train_loss_set.append(loss.item())\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # update parameters and take a step using the computed gradient\n",
    "            optimizer.step()\n",
    "\n",
    "            # update tracking variables\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += b_input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "\n",
    "        print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "        # Validation\n",
    "        # put model on evaluation mode to evaluate loss on the validation set\n",
    "        model.eval()\n",
    "\n",
    "        # tracking variables\n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "        # evaluate the data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "            # add batch to GPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            # unpack the inputs from dataloader\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            # telling the model not to compute or store gradients, saving memory and speed up validation\n",
    "            with torch.no_grad():\n",
    "                # forward pass, calculate logit predictions\n",
    "                logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            \n",
    "            # move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "        print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "    \n",
    "    return train_loss_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80b14e6c-5864-4c5c-9d89-023b8a84364d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|                                                                                           | 0/4 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SequenceClassifierOutput' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_loss_set \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 25\u001b[0m, in \u001b[0;36mtrain_and_validation\u001b[0;34m(train_dataloader, validation_dataloader, model, device, optimizer, epochs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m model(b_input_ids, token_type_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, attention_mask\u001b[38;5;241m=\u001b[39mb_input_mask, labels\u001b[38;5;241m=\u001b[39mb_labels)\n\u001b[0;32m---> 25\u001b[0m train_loss_set\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m())\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# backward pass\u001b[39;00m\n\u001b[1;32m     27\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SequenceClassifierOutput' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "train_loss_set = train_and_validation(train_dataloader, validation_dataloader, model, device, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86adc1e0-1798-4fb0-a0a8-7511a5ae0881",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
