{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bf01f6f-6a6d-4b2b-9282-24c1c366b6c4",
   "metadata": {},
   "source": [
    "### `preprocessing_and_embedding.ipynb`\n",
    "This jupyter notebook contains data set cleaning, tokenization and extracting embeddings (sentence vectors) for a suicidal tweet classifier.\n",
    "\n",
    "I tried to explain the important parts as much as I can.\n",
    "\n",
    "For the tokenization and embedding parts, I used the [BERT](https://huggingface.co/docs/transformers/model_doc/bert) model (specifically `bert-base-uncased` model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8644043-1c67-490f-8c63-38290d752d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import contractions\n",
    "import emoji\n",
    "\n",
    "import torch\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "stopword_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dc74b21-8eda-4e01-9248-a9b3dc982881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>vader_sentiment_label</th>\n",
       "      <th>vader_score</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.2699</td>\n",
       "      <td>Wow, my dad yday: â€œyou donâ€™t take those stupid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.5995</td>\n",
       "      <td>what part of this was really harmfult of a lot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3382</td>\n",
       "      <td>one of the ways I got through my #depression i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.8643</td>\n",
       "      <td>see i wanna do one of them but they all say th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.8316</td>\n",
       "      <td>IS IT clinical depression or is it the palpabl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  vader_sentiment_label  vader_score  \\\n",
       "0           0                      0      -0.2699   \n",
       "1           1                      0      -0.5995   \n",
       "2           2                      1       0.3382   \n",
       "3           3                      0      -0.8643   \n",
       "4           4                      0      -0.8316   \n",
       "\n",
       "                                               tweet  \n",
       "0  Wow, my dad yday: â€œyou donâ€™t take those stupid...  \n",
       "1  what part of this was really harmfult of a lot...  \n",
       "2  one of the ways I got through my #depression i...  \n",
       "3  see i wanna do one of them but they all say th...  \n",
       "4  IS IT clinical depression or is it the palpabl...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/vader_processed.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66e777bc-2755-48a9-8e55-9fde455869f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22963 entries, 0 to 22962\n",
      "Data columns (total 3 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   vader_sentiment_label  22963 non-null  int64  \n",
      " 1   vader_score            22963 non-null  float64\n",
      " 2   tweet                  22963 non-null  object \n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 538.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(['Unnamed: 0'], axis=1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142e8161-1a11-4bde-9882-664a49026e62",
   "metadata": {},
   "source": [
    "### Couple of things I noticed after the examination of the trimmed data:\n",
    "1) Most of the tweets contain hashtags, emojis, numbers, and symbols.\n",
    "2) The overall emotion of the tweet depends on what kind of emoji(s) does it contains. For example, the tweet that contains 'ðŸ˜ž' emoji is more likely to be depressive.\n",
    "3) Some of the punctiation marks are repetitive (i.e. '!!' or '??'). These marks could be valuable.\n",
    "4) There are non-English tweets.\n",
    "5) Most of the tweets contain tagged users (i.e. '@elon'). A column named 'mentions' in the original copy of the dataset provides the tagged users in the tweet however, not all of them appear in the column.\n",
    "6) Some of the tweets contain links and hardcoded pictures (i.e. 'pic.twitter.com/tBhxLdatP8'). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa538ef-9cde-4d70-97b1-b9247bcfac0b",
   "metadata": {},
   "source": [
    "### Some valuable attributes will be saved for later use:\n",
    "#### These attributes represent tweet's characteristic features. \n",
    "- Swearing or offensive word(s) [Integer] => How many are there?\n",
    "- Tweet length [Integer] => this column will be calculated after removing links.\n",
    "- Emoji attribute will be break down into two columns: pos_emoji [Boolean], neg_emoji [Boolean]\n",
    "- URL/Link [Boolean]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc5642d-80b9-4f77-8715-42598ae9ec86",
   "metadata": {},
   "source": [
    "### Tweets that contain depressive emojis are more likely to be depressive tweets.\n",
    "- So, all of the emojis in the dataset must be gathered into a data structure for later analysis.\n",
    "- Then, most common emojis must be found for categorization.\n",
    "- Finally, tweets should be checked whether they contain a categorized emoji or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "199a64c9-c8cb-4317-bbd9-5fc5c9da204c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ðŸ˜‚', [':face_with_tears_of_joy:', 460]),\n",
       " ('ðŸ˜­', [':loudly_crying_face:', 342]),\n",
       " ('â¤', [':red_heart:', 272]),\n",
       " ('ðŸ’”', [':broken_heart:', 175]),\n",
       " ('ðŸ˜”', [':pensive_face:', 132]),\n",
       " ('ðŸ»', [':light_skin_tone:', 127]),\n",
       " ('ðŸ¤£', [':rolling_on_the_floor_laughing:', 125]),\n",
       " ('â™€', [':female_sign:', 113]),\n",
       " ('ðŸ™', [':folded_hands:', 107]),\n",
       " ('ðŸ¼', [':medium-light_skin_tone:', 105]),\n",
       " ('ðŸ˜', [':smiling_face_with_heart-eyes:', 102]),\n",
       " ('ðŸ’œ', [':purple_heart:', 95]),\n",
       " ('ðŸ¾', [':medium-dark_skin_tone:', 91]),\n",
       " ('ðŸ™ƒ', [':upside-down_face:', 88]),\n",
       " ('ðŸ½', [':medium_skin_tone:', 88]),\n",
       " ('ðŸ¤·', [':person_shrugging:', 87]),\n",
       " ('ðŸ˜Š', [':smiling_face_with_smiling_eyes:', 76]),\n",
       " ('ðŸ˜©', [':weary_face:', 74]),\n",
       " ('ðŸ™„', [':face_with_rolling_eyes:', 72]),\n",
       " ('ðŸ’•', [':two_hearts:', 71]),\n",
       " ('ðŸ˜¢', [':crying_face:', 69]),\n",
       " ('â™‚', [':male_sign:', 68]),\n",
       " ('ðŸ¤”', [':thinking_face:', 63]),\n",
       " ('ðŸ¥º', [':pleading_face:', 61]),\n",
       " ('ðŸ˜ž', [':disappointed_face:', 60]),\n",
       " ('ðŸ’¯', [':hundred_points:', 59]),\n",
       " ('ðŸ–¤', [':black_heart:', 57]),\n",
       " ('ðŸ¤¦', [':person_facepalming:', 52]),\n",
       " ('ðŸ‘', [':clapping_hands:', 52]),\n",
       " ('ðŸ‘', [':thumbs_up:', 51]),\n",
       " ('ðŸ¤—', [':smiling_face_with_open_hands:', 44]),\n",
       " ('âœŒ', [':victory_hand:', 43]),\n",
       " ('ðŸ”¥', [':fire:', 43]),\n",
       " ('ðŸ¤ª', [':zany_face:', 42]),\n",
       " ('â˜º', [':smiling_face:', 41]),\n",
       " ('ðŸ˜…', [':grinning_face_with_sweat:', 41]),\n",
       " ('âœ¨', [':sparkles:', 41]),\n",
       " ('ðŸ˜ª', [':sleepy_face:', 38]),\n",
       " ('ðŸ˜Œ', [':relieved_face:', 38]),\n",
       " ('ðŸ’š', [':green_heart:', 37]),\n",
       " ('ðŸ˜Ž', [':smiling_face_with_sunglasses:', 36]),\n",
       " ('ðŸ˜’', [':unamused_face:', 35]),\n",
       " ('ðŸ’™', [':blue_heart:', 35]),\n",
       " ('ðŸ˜³', [':flushed_face:', 33]),\n",
       " ('ðŸ‘Œ', [':OK_hand:', 33]),\n",
       " ('ðŸ™Œ', [':raising_hands:', 31]),\n",
       " ('ðŸ¥´', [':woozy_face:', 31]),\n",
       " ('ðŸ¥°', [':smiling_face_with_hearts:', 31]),\n",
       " ('â™¥', [':heart_suit:', 31]),\n",
       " ('ðŸ˜', [':beaming_face_with_smiling_eyes:', 31]),\n",
       " ('ðŸ’–', [':sparkling_heart:', 30]),\n",
       " ('ðŸ’ª', [':flexed_biceps:', 30]),\n",
       " ('ðŸ’€', [':skull:', 29]),\n",
       " ('ðŸ˜˜', [':face_blowing_a_kiss:', 29]),\n",
       " ('âš¡', [':high_voltage:', 28]),\n",
       " ('ðŸ™‚', [':slightly_smiling_face:', 26]),\n",
       " ('â€¼', [':double_exclamation_mark:', 25]),\n",
       " ('ðŸ¤§', [':sneezing_face:', 25]),\n",
       " ('ðŸ˜«', [':tired_face:', 25]),\n",
       " ('ðŸ˜£', [':persevering_face:', 25]),\n",
       " ('ðŸŒ¸', [':cherry_blossom:', 23]),\n",
       " ('âœ…', [':check_mark_button:', 23]),\n",
       " ('ðŸŒˆ', [':rainbow:', 23]),\n",
       " ('ðŸ‘Š', [':oncoming_fist:', 22]),\n",
       " ('âœˆ', [':airplane:', 22]),\n",
       " ('ðŸ˜±', [':face_screaming_in_fear:', 22]),\n",
       " ('âž¡', [':right_arrow:', 21]),\n",
       " ('ðŸ˜•', [':confused_face:', 21]),\n",
       " ('ðŸ’—', [':growing_heart:', 20]),\n",
       " ('ðŸ˜¥', [':sad_but_relieved_face:', 20]),\n",
       " ('â„¢', [':trade_mark:', 20]),\n",
       " ('ðŸ¤¡', [':clown_face:', 20]),\n",
       " ('ðŸ˜‡', [':smiling_face_with_halo:', 19]),\n",
       " ('ðŸ‘‡', [':backhand_index_pointing_down:', 19]),\n",
       " ('ðŸ—£', [':speaking_head:', 19]),\n",
       " ('ðŸŽ¶', [':musical_notes:', 18]),\n",
       " ('ðŸ˜', [':neutral_face:', 18]),\n",
       " ('ðŸ¤™', [':call_me_hand:', 17]),\n",
       " ('â˜¹', [':frowning_face:', 17]),\n",
       " ('ðŸ˜“', [':downcast_face_with_sweat:', 17]),\n",
       " ('ðŸŒš', [':new_moon_face:', 17]),\n",
       " ('ðŸ’ž', [':revolving_hearts:', 16]),\n",
       " ('ðŸ‘', [':eye:', 16]),\n",
       " ('ðŸ’›', [':yellow_heart:', 16]),\n",
       " ('ðŸ’“', [':beating_heart:', 16]),\n",
       " ('ðŸ‘€', [':eyes:', 16]),\n",
       " ('â­', [':star:', 16]),\n",
       " ('ðŸ˜‰', [':winking_face:', 15]),\n",
       " ('ðŸ˜´', [':sleeping_face:', 15]),\n",
       " ('ðŸ˜¤', [':face_with_steam_from_nose:', 15]),\n",
       " ('ðŸ‘‹', [':waving_hand:', 15]),\n",
       " ('ðŸ¤', [':handshake:', 14]),\n",
       " ('ðŸ¤©', [':star-struck:', 14]),\n",
       " ('ðŸ˜‘', [':expressionless_face:', 14]),\n",
       " ('ðŸ˜–', [':confounded_face:', 14]),\n",
       " ('ðŸ¤¯', [':exploding_head:', 14]),\n",
       " ('âœ”', [':check_mark:', 14]),\n",
       " ('ðŸ¤˜', [':sign_of_the_horns:', 13]),\n",
       " ('ðŸ˜', [':squinting_face_with_tongue:', 13]),\n",
       " ('âœŠ', [':raised_fist:', 13]),\n",
       " ('ðŸ˜¬', [':grimacing_face:', 13]),\n",
       " ('âš½', [':soccer_ball:', 13]),\n",
       " ('ðŸ˜€', [':grinning_face:', 13]),\n",
       " ('ðŸ¥µ', [':hot_face:', 13]),\n",
       " ('ðŸ‘‰', [':backhand_index_pointing_right:', 13]),\n",
       " ('ðŸ™', [':slightly_frowning_face:', 12]),\n",
       " ('ðŸ¤­', [':face_with_hand_over_mouth:', 12]),\n",
       " ('ðŸ˜', [':smirking_face:', 12]),\n",
       " ('ðŸ¿', [':dark_skin_tone:', 11]),\n",
       " ('â¬‡', [':down_arrow:', 11]),\n",
       " ('ðŸŽµ', [':musical_note:', 11]),\n",
       " ('ðŸ§¡', [':orange_heart:', 11]),\n",
       " ('â£', [':heart_exclamation:', 11]),\n",
       " ('ðŸ™…', [':person_gesturing_NO:', 11]),\n",
       " ('ðŸ˜¡', [':enraged_face:', 11]),\n",
       " ('ðŸŒ¿', [':herb:', 10]),\n",
       " ('ðŸ™‹', [':person_raising_hand:', 10]),\n",
       " ('ðŸ‘…', [':tongue:', 10]),\n",
       " ('ðŸ˜„', [':grinning_face_with_smiling_eyes:', 10]),\n",
       " ('ðŸ¤Ÿ', [':love-you_gesture:', 9]),\n",
       " ('ðŸ˜œ', [':winking_face_with_tongue:', 9]),\n",
       " ('âš ', [':warning:', 9]),\n",
       " ('ðŸ’¡', [':light_bulb:', 9]),\n",
       " ('ðŸ™ˆ', [':see-no-evil_monkey:', 9]),\n",
       " ('ðŸ’˜', [':heart_with_arrow:', 9]),\n",
       " ('ðŸ˜†', [':grinning_squinting_face:', 9]),\n",
       " ('ðŸ§ ', [':brain:', 8]),\n",
       " ('ðŸ’…', [':nail_polish:', 8]),\n",
       " ('âœ‹', [':raised_hand:', 8]),\n",
       " ('ðŸ§', [':face_with_monocle:', 8]),\n",
       " ('ðŸ¤ ', [':cowboy_hat_face:', 8]),\n",
       " ('ðŸŽ‰', [':party_popper:', 8]),\n",
       " ('ðŸ’', [':person_tipping_hand:', 8]),\n",
       " ('ðŸ˜·', [':face_with_medical_mask:', 8]),\n",
       " ('â—¾', [':black_medium-small_square:', 8]),\n",
       " ('ðŸ’¥', [':collision:', 7]),\n",
       " ('ðŸ’†', [':person_getting_massage:', 7]),\n",
       " ('ðŸŒŸ', [':glowing_star:', 7]),\n",
       " ('ðŸ¤š', [':raised_back_of_hand:', 7]),\n",
       " ('ðŸ˜ ', [':angry_face:', 7]),\n",
       " ('ðŸ’Š', [':pill:', 7]),\n",
       " ('ðŸ¤ž', [':crossed_fingers:', 7]),\n",
       " ('ðŸ¤¬', [':face_with_symbols_on_mouth:', 7]),\n",
       " ('â˜•', [':hot_beverage:', 7]),\n",
       " ('ðŸ˜Ÿ', [':worried_face:', 7]),\n",
       " ('ðŸ¤‘', [':money-mouth_face:', 7]),\n",
       " ('ðŸš«', [':prohibited:', 7]),\n",
       " ('ðŸ‘©', [':woman:', 7]),\n",
       " ('ðŸ¤•', [':face_with_head-bandage:', 7]),\n",
       " ('ðŸ¤’', [':face_with_thermometer:', 7]),\n",
       " ('ðŸƒ', [':leaf_fluttering_in_wind:', 7]),\n",
       " ('ðŸ—¿', [':moai:', 7]),\n",
       " ('ðŸ’¤', [':ZZZ:', 7]),\n",
       " ('ðŸ‘Ž', [':thumbs_down:', 6]),\n",
       " ('ðŸ¤®', [':face_vomiting:', 6]),\n",
       " ('ðŸ’', [':heart_with_ribbon:', 6]),\n",
       " ('ðŸ’‹', [':kiss_mark:', 6]),\n",
       " ('ðŸ¤–', [':robot:', 6]),\n",
       " ('ðŸ˜¶', [':face_without_mouth:', 6]),\n",
       " ('ðŸ˜µ', [':face_with_crossed-out_eyes:', 6]),\n",
       " ('ðŸš€', [':rocket:', 6]),\n",
       " ('ðŸ¤¸', [':person_cartwheeling:', 6]),\n",
       " ('âšª', [':white_circle:', 6]),\n",
       " ('ðŸ˜ƒ', [':grinning_face_with_big_eyes:', 5]),\n",
       " ('ðŸ˜¹', [':cat_with_tears_of_joy:', 5]),\n",
       " ('ðŸ¥€', [':wilted_flower:', 5]),\n",
       " ('ðŸ¦‹', [':butterfly:', 5]),\n",
       " ('ðŸŒ±', [':seedling:', 5]),\n",
       " ('ðŸ’»', [':laptop:', 5]),\n",
       " ('ðŸ¤“', [':nerd_face:', 5]),\n",
       " ('ðŸ˜°', [':anxious_face_with_sweat:', 5]),\n",
       " ('ðŸ’«', [':dizzy:', 5]),\n",
       " ('ðŸ’¦', [':sweat_droplets:', 5]),\n",
       " ('ðŸ¤¨', [':face_with_raised_eyebrow:', 5]),\n",
       " ('ðŸ˜¯', [':hushed_face:', 5]),\n",
       " ('ðŸš¨', [':police_car_light:', 4]),\n",
       " ('â¤µ', [':right_arrow_curving_down:', 4]),\n",
       " ('ðŸ˜ˆ', [':smiling_face_with_horns:', 4]),\n",
       " ('â“', [':red_question_mark:', 4]),\n",
       " ('â˜€', [':sun:', 4]),\n",
       " ('ðŸ’ƒ', [':woman_dancing:', 4]),\n",
       " ('ðŸ’­', [':thought_balloon:', 4]),\n",
       " ('â˜', [':index_pointing_up:', 4]),\n",
       " ('ðŸ“ž', [':telephone_receiver:', 4]),\n",
       " ('ðŸ‘¿', [':angry_face_with_horns:', 4]),\n",
       " ('ðŸ˜›', [':face_with_tongue:', 4]),\n",
       " ('ðŸ˜¨', [':fearful_face:', 4]),\n",
       " ('ðŸ“·', [':camera:', 4]),\n",
       " ('ðŸŒ»', [':sunflower:', 4]),\n",
       " ('ðŸ¦¡', [':badger:', 4]),\n",
       " ('ðŸ™‡', [':person_bowing:', 4]),\n",
       " ('ðŸ”´', [':red_circle:', 4]),\n",
       " ('ðŸŒ¹', [':rose:', 4]),\n",
       " ('ðŸ––', [':vulcan_salute:', 4]),\n",
       " ('ðŸ»', [':clinking_beer_mugs:', 4]),\n",
       " ('ðŸ˜¿', [':crying_cat:', 4]),\n",
       " ('ðŸ–•', [':middle_finger:', 3]),\n",
       " ('ðŸ¥Š', [':boxing_glove:', 3]),\n",
       " ('ðŸ˜š', [':kissing_face_with_closed_eyes:', 3]),\n",
       " ('ðŸ§˜', [':person_in_lotus_position:', 3]),\n",
       " ('ðŸ™†', [':person_gesturing_OK:', 3]),\n",
       " ('ðŸ“', [':memo:', 3]),\n",
       " ('âœ', [':writing_hand:', 3]),\n",
       " ('âŒ', [':cross_mark:', 3]),\n",
       " ('ðŸ˜', [':elephant:', 3]),\n",
       " ('ðŸ“–', [':open_book:', 3]),\n",
       " ('ðŸ“¸', [':camera_with_flash:', 3]),\n",
       " ('ðŸŽ­', [':performing_arts:', 3]),\n",
       " ('ðŸ”²', [':black_square_button:', 3]),\n",
       " ('ðŸŒ¼', [':blossom:', 3]),\n",
       " ('ðŸ•Š', [':dove:', 3]),\n",
       " ('ðŸ¤¢', [':nauseated_face:', 3]),\n",
       " ('ðŸ˜™', [':kissing_face_with_smiling_eyes:', 3]),\n",
       " ('ðŸš¶', [':person_walking:', 3]),\n",
       " ('ðŸ§€', [':cheese_wedge:', 3]),\n",
       " ('Â®', [':registered:', 3]),\n",
       " ('ðŸŒ•', [':full_moon:', 3]),\n",
       " ('ðŸ†', [':trophy:', 3]),\n",
       " ('ðŸŒ§', [':cloud_with_rain:', 3]),\n",
       " ('ðŸ“¹', [':video_camera:', 3]),\n",
       " ('ðŸŒŠ', [':water_wave:', 3]),\n",
       " ('ðŸŒŽ', [':globe_showing_Americas:', 3]),\n",
       " ('â™Œ', [':Leo:', 3]),\n",
       " ('ðŸ˜®', [':face_with_open_mouth:', 3]),\n",
       " ('ðŸ‘¼', [':baby_angel:', 3]),\n",
       " ('ðŸ“º', [':television:', 3]),\n",
       " ('ðŸ¦ ', [':microbe:', 3]),\n",
       " ('ðŸ’©', [':pile_of_poo:', 3]),\n",
       " ('ðŸ•º', [':man_dancing:', 3]),\n",
       " ('ðŸ´', [':black_flag:', 3]),\n",
       " ('Â©', [':copyright:', 3]),\n",
       " ('ðŸŒž', [':sun_with_face:', 3]),\n",
       " ('ðŸ³', [':white_flag:', 3]),\n",
       " ('â­•', [':hollow_red_circle:', 3]),\n",
       " ('ðŸ“£', [':megaphone:', 2]),\n",
       " ('ðŸ¥‹', [':martial_arts_uniform:', 2]),\n",
       " ('ðŸŽ†', [':fireworks:', 2]),\n",
       " ('ðŸ€', [':four_leaf_clover:', 2]),\n",
       " ('ðŸƒ', [':person_running:', 2]),\n",
       " ('ðŸ•', [':pizza:', 2]),\n",
       " ('ðŸ¾', [':paw_prints:', 2]),\n",
       " ('ðŸ“©', [':envelope_with_arrow:', 2]),\n",
       " ('ðŸŒ', [':globe_showing_Europe-Africa:', 2]),\n",
       " ('ðŸŽ§', [':headphone:', 2]),\n",
       " ('â—', [':red_exclamation_mark:', 2]),\n",
       " ('ðŸ˜—', [':kissing_face:', 2]),\n",
       " ('â˜‘', [':check_box_with_check:', 2]),\n",
       " ('ðŸ¶', [':dog_face:', 2]),\n",
       " ('ðŸ‘‘', [':crown:', 2]),\n",
       " ('ðŸ˜‹', [':face_savoring_food:', 2]),\n",
       " ('ðŸ’', [':cherries:', 2]),\n",
       " ('ðŸ»', [':bear:', 2]),\n",
       " ('ðŸ‘´', [':old_man:', 2]),\n",
       " ('ðŸ§–', [':person_in_steamy_room:', 2]),\n",
       " ('ðŸ‘¨', [':man:', 2]),\n",
       " ('ðŸ‘¹', [':ogre:', 2]),\n",
       " ('ðŸ¤¥', [':lying_face:', 2]),\n",
       " ('ðŸ’‰', [':syringe:', 2]),\n",
       " ('ðŸŽª', [':circus_tent:', 2]),\n",
       " ('âœ', [':latin_cross:', 2]),\n",
       " ('ðŸ˜§', [':anguished_face:', 2]),\n",
       " ('ðŸ¥³', [':partying_face:', 2]),\n",
       " ('ðŸ³', [':cooking:', 2]),\n",
       " ('ðŸŽ¤', [':microphone:', 2]),\n",
       " ('ðŸ–', [':hand_with_fingers_splayed:', 2]),\n",
       " ('ðŸ”ª', [':kitchen_knife:', 2]),\n",
       " ('ðŸ‘½', [':alien:', 2]),\n",
       " ('ðŸ§¢', [':billed_cap:', 2]),\n",
       " ('â˜', [':cloud:', 2]),\n",
       " ('ðŸ¤²', [':palms_up_together:', 2]),\n",
       " ('ðŸ˜»', [':smiling_cat_with_heart-eyes:', 2]),\n",
       " ('â˜„', [':comet:', 2]),\n",
       " ('ðŸ’', [':bouquet:', 2]),\n",
       " ('ðŸ’°', [':money_bag:', 2]),\n",
       " ('ðŸ¥‘', [':avocado:', 2]),\n",
       " ('ðŸ–', [':beach_with_umbrella:', 2]),\n",
       " ('ðŸ—½', [':Statue_of_Liberty:', 2]),\n",
       " ('ðŸ’¬', [':speech_balloon:', 2]),\n",
       " ('ðŸ–', [':pig:', 2]),\n",
       " ('ðŸ“±', [':mobile_phone:', 2]),\n",
       " ('â¬†', [':up_arrow:', 2]),\n",
       " ('ðŸ‘»', [':ghost:', 2]),\n",
       " ('ðŸ¡', [':blowfish:', 2]),\n",
       " ('ðŸŽ®', [':video_game:', 2]),\n",
       " ('ðŸ‘¾', [':alien_monster:', 2]),\n",
       " ('ðŸ´', [':fork_and_knife:', 2]),\n",
       " ('ðŸ”«', [':water_pistol:', 2]),\n",
       " ('ðŸ‘¸', [':princess:', 2]),\n",
       " ('â˜ ', [':skull_and_crossbones:', 2]),\n",
       " ('ðŸŽ™', [':studio_microphone:', 2]),\n",
       " ('ðŸŽ¨', [':artist_palette:', 2]),\n",
       " ('ðŸ¾', [':bottle_with_popping_cork:', 2]),\n",
       " ('ðŸ§”', [':person_beard:', 2]),\n",
       " ('ðŸ•¸', [':spider_web:', 2]),\n",
       " ('â˜”', [':umbrella_with_rain_drops:', 2]),\n",
       " ('ðŸ”œ', [':SOON_arrow:', 2]),\n",
       " ('â›ˆ', [':cloud_with_lightning_and_rain:', 2]),\n",
       " ('ðŸŸ', [':french_fries:', 2]),\n",
       " ('ðŸ„', [':cow:', 1]),\n",
       " ('ðŸ¥›', [':glass_of_milk:', 1]),\n",
       " ('ðŸ…', [':tomato:', 1]),\n",
       " ('ðŸ¸', [':frog:', 1]),\n",
       " ('ðŸŽ‡', [':sparkler:', 1]),\n",
       " ('ðŸ™', [':person_frowning:', 1]),\n",
       " ('ðŸ¥‚', [':clinking_glasses:', 1]),\n",
       " ('ðŸŒ', [':banana:', 1]),\n",
       " ('ðŸŒ¬', [':wind_face:', 1]),\n",
       " ('ðŸ', [':goat:', 1]),\n",
       " ('ðŸ˜¾', [':pouting_cat:', 1]),\n",
       " ('ðŸ€', [':basketball:', 1]),\n",
       " ('ðŸš¬', [':cigarette:', 1]),\n",
       " ('ðŸ—¨', [':left_speech_bubble:', 1]),\n",
       " ('ðŸ¿', [':chipmunk:', 1]),\n",
       " ('ðŸŒ´', [':palm_tree:', 1]),\n",
       " ('ðŸŒ·', [':tulip:', 1]),\n",
       " ('ðŸ”„', [':counterclockwise_arrows_button:', 1]),\n",
       " ('ðŸ¤', [':zipper-mouth_face:', 1]),\n",
       " ('ðŸšª', [':door:', 1]),\n",
       " ('ðŸ§¿', [':nazar_amulet:', 1]),\n",
       " ('ðŸŠ', [':tangerine:', 1]),\n",
       " ('ðŸ¤´', [':prince:', 1]),\n",
       " ('ðŸ¥£', [':bowl_with_spoon:', 1]),\n",
       " ('ðŸŸ', [':fish:', 1]),\n",
       " ('ðŸ–‹', [':fountain_pen:', 1]),\n",
       " ('ðŸˆ', [':cat:', 1]),\n",
       " ('ðŸ±', [':cat_face:', 1]),\n",
       " ('ðŸ—¡', [':dagger:', 1]),\n",
       " ('ðŸš¹', [':menâ€™s_room:', 1]),\n",
       " ('ðŸšº', [':womenâ€™s_room:', 1]),\n",
       " ('ðŸŒ©', [':cloud_with_lightning:', 1]),\n",
       " ('ðŸ£', [':hatching_chick:', 1]),\n",
       " ('ðŸ‘„', [':mouth:', 1]),\n",
       " ('ðŸ‘ƒ', [':nose:', 1]),\n",
       " ('ðŸŽ¥', [':movie_camera:', 1]),\n",
       " ('ðŸŽ¡', [':ferris_wheel:', 1]),\n",
       " ('ðŸ¤œ', [':right-facing_fist:', 1]),\n",
       " ('ðŸ˜²', [':astonished_face:', 1]),\n",
       " ('ðŸ••', [':six_oâ€™clock:', 1]),\n",
       " ('ðŸ”', [':locked_with_key:', 1]),\n",
       " ('ðŸ¤¤', [':drooling_face:', 1]),\n",
       " ('ðŸ‘£', [':footprints:', 1]),\n",
       " ('ðŸ‘‚', [':ear:', 1]),\n",
       " ('ðŸŒ', [':globe_with_meridians:', 1]),\n",
       " ('ðŸŽ¸', [':guitar:', 1]),\n",
       " ('ðŸ’¨', [':dashing_away:', 1]),\n",
       " ('ðŸ’´', [':yen_banknote:', 1]),\n",
       " ('ðŸš—', [':automobile:', 1]),\n",
       " ('ðŸŒ¨', [':cloud_with_snow:', 1]),\n",
       " ('ðŸ•¯', [':candle:', 1]),\n",
       " ('ðŸ¦„', [':unicorn:', 1]),\n",
       " ('â—¼', [':black_medium_square:', 1]),\n",
       " ('ðŸº', [':wolf:', 1]),\n",
       " ('âš•', [':medical_symbol:', 1]),\n",
       " ('ðŸ§¤', [':gloves:', 1]),\n",
       " ('ðŸŒ™', [':crescent_moon:', 1]),\n",
       " ('ðŸ', [':chequered_flag:', 1]),\n",
       " ('ðŸŒ‘', [':new_moon:', 1]),\n",
       " ('ðŸ›', [':bathtub:', 1]),\n",
       " ('ðŸŽ“', [':graduation_cap:', 1]),\n",
       " ('ðŸ¥œ', [':peanuts:', 1]),\n",
       " ('â–¶', [':play_button:', 1]),\n",
       " ('â˜‚', [':umbrella:', 1]),\n",
       " ('ðŸ§»', [':roll_of_paper:', 1]),\n",
       " ('ðŸŽ¯', [':bullseye:', 1]),\n",
       " ('ðŸŒµ', [':cactus:', 1]),\n",
       " ('ðŸŒª', [':tornado:', 1]),\n",
       " ('ðŸ’', [':ring:', 1]),\n",
       " ('ðŸ‘ˆ', [':backhand_index_pointing_left:', 1]),\n",
       " ('ðŸ›¡', [':shield:', 1]),\n",
       " ('ðŸ ', [':tropical_fish:', 1]),\n",
       " ('ðŸ…¾', [':O_button_(blood_type):', 1]),\n",
       " ('â“‚', [':circled_M:', 1]),\n",
       " ('ðŸ…±', [':B_button_(blood_type):', 1]),\n",
       " ('ðŸ“‰', [':chart_decreasing:', 1]),\n",
       " ('ðŸ‘°', [':person_with_veil:', 1]),\n",
       " ('ðŸ’Œ', [':love_letter:', 1]),\n",
       " ('ðŸ“²', [':mobile_phone_with_arrow:', 1]),\n",
       " ('ðŸŒ', [':globe_showing_Asia-Australia:', 1]),\n",
       " ('ðŸ°', [':rabbit_face:', 1]),\n",
       " ('ðŸ½', [':fork_and_knife_with_plate:', 1]),\n",
       " ('ðŸ“š', [':books:', 1]),\n",
       " ('ðŸ“¢', [':loudspeaker:', 1]),\n",
       " ('ðŸ§œ', [':merperson:', 1]),\n",
       " ('ðŸ§Ÿ', [':zombie:', 1]),\n",
       " ('âš°', [':coffin:', 1]),\n",
       " ('ðŸ†˜', [':SOS_button:', 1]),\n",
       " ('â™¾', [':infinity:', 1]),\n",
       " ('ðŸŽ«', [':ticket:', 1]),\n",
       " ('ðŸ›‘', [':stop_sign:', 1]),\n",
       " ('ðŸ¥„', [':spoon:', 1]),\n",
       " ('ðŸ‘«', [':woman_and_man_holding_hands:', 1]),\n",
       " ('ðŸ‘Ÿ', [':running_shoe:', 1]),\n",
       " ('ðŸ—‘', [':wastebasket:', 1]),\n",
       " ('âœ’', [':black_nib:', 1]),\n",
       " ('ðŸšŒ', [':bus:', 1]),\n",
       " ('ðŸš†', [':train:', 1]),\n",
       " ('ðŸš‡', [':metro:', 1]),\n",
       " ('ðŸš˜', [':oncoming_automobile:', 1]),\n",
       " ('â›…', [':sun_behind_cloud:', 1]),\n",
       " ('ðŸ”‘', [':key:', 1]),\n",
       " ('ðŸ’§', [':droplet:', 1]),\n",
       " ('ðŸ’Ž', [':gem_stone:', 1]),\n",
       " ('ðŸ', [':green_apple:', 1]),\n",
       " ('ðŸ¥•', [':carrot:', 1]),\n",
       " ('ðŸ“', [':strawberry:', 1]),\n",
       " ('ðŸ‘†', [':backhand_index_pointing_up:', 1]),\n",
       " ('âœ‚', [':scissors:', 1]),\n",
       " ('ðŸŒ', [':full_moon_face:', 1]),\n",
       " ('ðŸ¦‡', [':bat:', 1]),\n",
       " ('ðŸŽ', [':red_apple:', 1]),\n",
       " ('ðŸ‘º', [':goblin:', 1]),\n",
       " ('âœ', [':pencil:', 1]),\n",
       " ('ðŸ›«', [':airplane_departure:', 1]),\n",
       " ('ðŸŒº', [':hibiscus:', 1]),\n",
       " ('ðŸ”µ', [':blue_circle:', 1]),\n",
       " ('ðŸŒ€', [':cyclone:', 1]),\n",
       " ('ðŸ', [':cricket_game:', 1])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_attributes = df.copy()\n",
    "emoji_map = {}\n",
    "\n",
    "def get_emoji_map(tweet, map):\n",
    "    if type(tweet) == 'float64':\n",
    "        print(tweet)\n",
    "    for char in tweet:\n",
    "        if emoji.is_emoji(char):\n",
    "            if char not in map.keys():\n",
    "                map[char] = [emoji.demojize(char), 1]\n",
    "            else:\n",
    "                map[char][1] += 1\n",
    "                \n",
    "df_attributes['tweet'].apply(get_emoji_map, map=emoji_map)\n",
    "sorted_emoji_map = sorted(emoji_map.items(), key=lambda x:x[1][1], reverse=True)\n",
    "sorted_emoji_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d315c6a5-4b91-4ea1-800a-07970380ea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I exract positive and negative emojis using the analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "emoji_df = pd.DataFrame(columns=['emoji', 'emoji_unicode', 'emoji_count', 'pos_score', 'neg_score', 'neu_score', 'compound_score'])\n",
    "\n",
    "for key, val in emoji_map.items():\n",
    "    emoji_df.loc[len(emoji_df.index)] = [key, val[0], val[1], analyzer.polarity_scores(key)['pos'], analyzer.polarity_scores(key)['neg'], analyzer.polarity_scores(key)['neu'], analyzer.polarity_scores(key)['compound']]\n",
    "\n",
    "pos_df = emoji_df[emoji_df['compound_score'] > 0.27]\n",
    "neg_df = emoji_df[emoji_df['compound_score'] < -0.27]\n",
    "\n",
    "pos_emojis = pos_df['emoji_unicode'].tolist()\n",
    "neg_emojis = neg_df['emoji_unicode'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aae32fe3-3dbf-4d03-b3f4-e6b0274e86f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_url_link(tweet):\n",
    "    sentence = tweet.split(' ')\n",
    "    for word in sentence:\n",
    "        if word.startswith('https:') or word.startswith('http:'): \n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def check_pos_emoji(tweet):\n",
    "    for char in tweet:\n",
    "        if emoji.is_emoji(char) and char in pos_emojis:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def check_neg_emoji(tweet):\n",
    "    for char in tweet:\n",
    "        if emoji.is_emoji(char) and char in neg_emojis:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def get_tweet_length(tweet):\n",
    "    sentence = tweet.split(' ')\n",
    "    res = \" \"\n",
    "    \n",
    "    for word in sentence:\n",
    "        if word.startswith('https:') or word.startswith('http:') or word.startswith('pic.twitter.com'):\n",
    "            sentence.remove(word)\n",
    "    return len(res.join(sentence))\n",
    "\n",
    "def get_profanity_words(tweet):\n",
    "    cleaned_tweet = re.sub(r'[^\\w\\s]', '', tweet)\n",
    "    sentence = cleaned_tweet.split(' ') \n",
    "    profanity_wordlist = np.loadtxt('../data/profanity_wordlist.txt', dtype='str')\n",
    "    count = 0\n",
    "    \n",
    "    for word in sentence:\n",
    "        if word.lower() in profanity_wordlist:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e368411-a0ba-4e29-bba5-756adc9c47df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've been trying to figure out why my depression is so damn hard to shake today. I knew something was going on. - 0\n",
      "\n",
      "url link: 0\n",
      "positive emoji: 0\n",
      "negative emoji: 0\n",
      "tweet length: 111\n",
      "profanity word: 1\n"
     ]
    }
   ],
   "source": [
    "tweet_num = 22913\n",
    "\n",
    "tweet = df_attributes['tweet'][tweet_num]\n",
    "vader_score = df_attributes['vader_sentiment_label'][tweet_num]\n",
    "\n",
    "print(f\"{tweet} - {vader_score}\\n\")\n",
    "print(f\"url link: {check_url_link(tweet)}\")\n",
    "print(f\"positive emoji: {check_pos_emoji(tweet)}\")\n",
    "print(f\"negative emoji: {check_neg_emoji(tweet)}\")\n",
    "print(f\"tweet length: {get_tweet_length(tweet)}\")\n",
    "print(f\"profanity word: {get_profanity_words(tweet)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "582ba9d1-a8fd-4060-a7b5-ca55926b2608",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attributes['tweet_length'] = df_attributes['tweet'].apply(get_tweet_length)\n",
    "df_attributes['url_link'] = df_attributes['tweet'].apply(check_url_link)\n",
    "df_attributes['pos_emoji'] = df_attributes['tweet'].apply(check_pos_emoji)\n",
    "df_attributes['neg_emoji'] = df_attributes['tweet'].apply(check_neg_emoji)\n",
    "df_attributes['profanity_word'] = df_attributes['tweet'].apply(get_profanity_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "15e63296-2be2-4bca-8d48-0af631f12407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vader_sentiment_label</th>\n",
       "      <th>vader_score</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_length</th>\n",
       "      <th>url_link</th>\n",
       "      <th>pos_emoji</th>\n",
       "      <th>neg_emoji</th>\n",
       "      <th>profanity_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.2699</td>\n",
       "      <td>Wow, my dad yday: â€œyou donâ€™t take those stupid...</td>\n",
       "      <td>278</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.5995</td>\n",
       "      <td>what part of this was really harmfult of a lot...</td>\n",
       "      <td>274</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.3382</td>\n",
       "      <td>one of the ways I got through my #depression i...</td>\n",
       "      <td>208</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.8643</td>\n",
       "      <td>see i wanna do one of them but they all say th...</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.8316</td>\n",
       "      <td>IS IT clinical depression or is it the palpabl...</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22958</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.8126</td>\n",
       "      <td>CBD for depression? Nature works in mysterious...</td>\n",
       "      <td>116</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22959</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.5719</td>\n",
       "      <td>Depression is real</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22960</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.5060</td>\n",
       "      <td>Even though Tropical Depression Barry did not ...</td>\n",
       "      <td>245</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22961</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.7906</td>\n",
       "      <td>https://medtally.com/post/cluster-analysis-wi...</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22962</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.7783</td>\n",
       "      <td>New clinical trial for #depression: Task Shift...</td>\n",
       "      <td>127</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22963 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       vader_sentiment_label  vader_score  \\\n",
       "0                          0      -0.2699   \n",
       "1                          0      -0.5995   \n",
       "2                          1       0.3382   \n",
       "3                          0      -0.8643   \n",
       "4                          0      -0.8316   \n",
       "...                      ...          ...   \n",
       "22958                      0      -0.8126   \n",
       "22959                      0      -0.5719   \n",
       "22960                      0      -0.5060   \n",
       "22961                      0      -0.7906   \n",
       "22962                      0      -0.7783   \n",
       "\n",
       "                                                   tweet  tweet_length  \\\n",
       "0      Wow, my dad yday: â€œyou donâ€™t take those stupid...           278   \n",
       "1      what part of this was really harmfult of a lot...           274   \n",
       "2      one of the ways I got through my #depression i...           208   \n",
       "3      see i wanna do one of them but they all say th...           114   \n",
       "4      IS IT clinical depression or is it the palpabl...            78   \n",
       "...                                                  ...           ...   \n",
       "22958  CBD for depression? Nature works in mysterious...           116   \n",
       "22959                                 Depression is real            18   \n",
       "22960  Even though Tropical Depression Barry did not ...           245   \n",
       "22961   https://medtally.com/post/cluster-analysis-wi...            83   \n",
       "22962  New clinical trial for #depression: Task Shift...           127   \n",
       "\n",
       "       url_link  pos_emoji  neg_emoji  profanity_word  \n",
       "0             0          0          0               0  \n",
       "1             0          0          0               0  \n",
       "2             0          0          0               0  \n",
       "3             0          0          0               0  \n",
       "4             0          0          0               0  \n",
       "...         ...        ...        ...             ...  \n",
       "22958         1          0          0               0  \n",
       "22959         0          0          0               0  \n",
       "22960         1          0          0               0  \n",
       "22961         1          0          0               0  \n",
       "22962         1          0          0               0  \n",
       "\n",
       "[22963 rows x 8 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9529b1ed-b0bc-4065-bb9e-48de583e22b7",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "Data should be processed in a way that it can be vectorized for tokenization.\n",
    "\n",
    "(Note: At this point, not so sure about removing hashtags. Maybe will delete later)\n",
    "\n",
    "- All tweets converted into lowercase.\n",
    "- URL(s) removed from tweets. Such as, \"https...., pic.twitter.com/...\"\n",
    "- All contractions are expanded.\n",
    "- Accented chars are converted into original forms. Such as, \"Ã¡\": \"a\"\n",
    "- All emojis, mentions and digits are removed.\n",
    "- All punctuation marks and special characters (i.e. Â£, $) are removed.\n",
    "- All stopwords are removed.\n",
    "- Finally, stemming is applied for a more robust data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e22ac738-6268-4d3c-a184-11d24b94959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_attributes.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c606bf02-8c79-4b8b-8ae5-74a10302b008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(tweet):\n",
    "    return tweet.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "412fd8b6-25cc-4598-8356-40e6fc22cfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove url links such as, 'https://...', 'http://...', or 'pic.twitter...'\n",
    "def remove_url(tweet):\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)\n",
    "    tweet = re.sub(r'pic\\.twitter\\.com\\S+', '', tweet)\n",
    "    tweet = re.sub(r'www.+', '', tweet)\n",
    "    tweet = tweet.replace(u'\\xa0', u' ')\n",
    "    \n",
    "    return tweet.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c3ee7195-d03e-46f7-b611-0201713f03a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this dictionary contains contractions and their expanded forms\n",
    "contractions_dict = {\n",
    "  \"brb\": \"be right back\",\n",
    "  \"btw\": \"by the way\",\n",
    "  \"cant\": \"can not\",\n",
    "  \"dont\": \"do not\",\n",
    "  \"doesnt\": \"doesn ot\",\n",
    "  \"didnt\": \"did not\",\n",
    "  \"hasnt\": \"has not\",\n",
    "  \"havent\": \"have not\",\n",
    "  \"heres\": \"here is\",\n",
    "  \"howd\": \"how did\",\n",
    "  \"howve\": \"how have\",  \n",
    "  \"hows\": \"how is\",\n",
    "  \"id\": \"i would\",\n",
    "  \"ive\": \"i have\",\n",
    "  \"isnt\": \"is not\",\n",
    "  \"itd\": \"it would\",\n",
    "  \"itll\": \"it will\",\n",
    "  \"its\": \"it iss\",\n",
    "  \"kys\": \"kill yourself\",  \n",
    "  \"lets\": \"let us\", \n",
    "  \"ngl\": \"not gonna lie\",\n",
    "  \"omg\": \"oh my god\",\n",
    "  \"omfg\": \"oh my fucking god\",  \n",
    "  \"shes\": \"she is\",\n",
    "  \"stfu\": \"shut the fuck up\",  \n",
    "  \"thats\": \"that is\",\n",
    "  \"theres\": \"there is\",\n",
    "  \"theyd\": \"they would\",\n",
    "  \"theyll\": \"they will\",\n",
    "  \"theyre\": \"they are\",\n",
    "  \"theyve\": \"they have\",\n",
    "  \"thisll\": \"this will\",  \n",
    "  \"uve\": \"you have\",  \n",
    "  \"wasnt\": \"was not\",\n",
    "  \"wed\": \"we would\",\n",
    "  \"werent\": \"were not\",\n",
    "  \"whatll\": \"what will\",\n",
    "  \"what're\": \"what are\",\n",
    "  \"whats\": \"what is\",\n",
    "  \"whatve\": \"what have\",\n",
    "  \"whens\": \"when is\",\n",
    "  \"whered\": \"where would\",\n",
    "  \"wheres\": \"where is\",\n",
    "  \"whereve\": \"where have\",\n",
    "  \"wholl\": \"who will\",\n",
    "  \"who'll've\": \"who will have\",\n",
    "  \"whos\": \"who is\",\n",
    "  \"whove\": \"who have\",\n",
    "  \"whys\": \"why is\",\n",
    "  \"whyve\": \"why have\",\n",
    "  \"will've\": \"will have\",\n",
    "  \"wont\": \"will not\",\n",
    "  \"wouldve\": \"would have\",\n",
    "  \"wouldnt\": \"would not\",\n",
    "  \"yall\": \"you all\",\n",
    "  \"yalls\": \"you alls\",\n",
    "  \"youd\": \"you would\",\n",
    "  \"youll\": \"you will\",\n",
    "  \"youllve\": \"you will have\",\n",
    "  \"youre\": \"you are\",\n",
    "  \"youve\": \"you have\",\n",
    "  \"ain't\": \"am not\",\n",
    "  \"aren't\": \"are not\",\n",
    "  \"can't\": \"cannot\",\n",
    "  \"can't've\": \"cannot have\",\n",
    "  \"'cause\": \"because\",\n",
    "  \"could've\": \"could have\",\n",
    "  \"couldn't\": \"could not\",\n",
    "  \"couldn't've\": \"could not have\",\n",
    "  \"didn't\": \"did not\",\n",
    "  \"doesn't\": \"does not\",\n",
    "  \"don't\": \"do not\",\n",
    "  \"hadn't\": \"had not\",\n",
    "  \"hadn't've\": \"had not have\",\n",
    "  \"hasn't\": \"has not\",\n",
    "  \"haven't\": \"have not\",\n",
    "  \"he'd\": \"he would\",\n",
    "  \"he'd've\": \"he would have\",\n",
    "  \"he'll\": \"he will\",\n",
    "  \"he'll've\": \"he will have\",\n",
    "  \"he's\": \"he is\",\n",
    "  \"here's\": \"here is\", \n",
    "  \"how'd\": \"how did\",\n",
    "  \"how'd'y\": \"how do you\",\n",
    "  \"how've\": \"how have\",  \n",
    "  \"how'll\": \"how will\",\n",
    "  \"how's\": \"how is\",\n",
    "  \"i'd\": \"i would\",\n",
    "  \"i'd've\": \"i would have\",\n",
    "  \"i'll\": \"i will\",\n",
    "  \"i'll've\": \"i will have\",\n",
    "  \"i'm\": \"i am\",\n",
    "  \"i've\": \"i have\",\n",
    "  \"isn't\": \"is not\",\n",
    "  \"it'd\": \"it had\",\n",
    "  \"it'd've\": \"it would have\",\n",
    "  \"it'll\": \"it will\",\n",
    "  \"it'll've\": \"it will have\",\n",
    "  \"it's\": \"it is\",\n",
    "  \"let's\": \"let us\",\n",
    "  \"ma'am\": \"madam\",\n",
    "  \"mayn't\": \"may not\",\n",
    "  \"might've\": \"might have\",\n",
    "  \"mightn't\": \"might not\",\n",
    "  \"mightn't've\": \"might not have\",\n",
    "  \"must've\": \"must have\",\n",
    "  \"mustn't\": \"must not\",\n",
    "  \"mustn't've\": \"must not have\",\n",
    "  \"needn't\": \"need not\",\n",
    "  \"needn't've\": \"need not have\",\n",
    "  \"o'clock\": \"of the clock\",\n",
    "  \"oughtn't\": \"ought not\",\n",
    "  \"oughtn't've\": \"ought not have\",\n",
    "  \"seen't\": \"see not\",  \n",
    "  \"shan't\": \"shall not\",\n",
    "  \"sha'n't\": \"shall not\",\n",
    "  \"shan't've\": \"shall not have\",\n",
    "  \"she'd\": \"she would\",\n",
    "  \"she'd've\": \"she would have\",\n",
    "  \"she'll\": \"she will\",\n",
    "  \"she'll've\": \"she will have\",\n",
    "  \"she's\": \"she is\",\n",
    "  \"should've\": \"should have\",\n",
    "  \"shouldn't\": \"should not\",\n",
    "  \"shouldn't've\": \"should not have\",\n",
    "  \"so've\": \"so have\",\n",
    "  \"so's\": \"so is\",\n",
    "  \"that'd\": \"that would\",\n",
    "  \"that'd've\": \"that would have\",\n",
    "  \"that'll\": \"that will\", \n",
    "  \"that's\": \"that is\",\n",
    "  \"there'd\": \"there had\",\n",
    "  \"there'd've\": \"there would have\",\n",
    "  \"there's\": \"there is\",\n",
    "  \"they'd\": \"they would\",\n",
    "  \"they'd've\": \"they would have\",\n",
    "  \"they'll\": \"they will\",\n",
    "  \"they'll've\": \"they will have\",\n",
    "  \"they're\": \"they are\",\n",
    "  \"they've\": \"they have\",\n",
    "  \"this'll\": \"this will\",  \n",
    "  \"to've\": \"to have\",\n",
    "  \"u've\": \"you have\",  \n",
    "  \"wasn't\": \"was not\",\n",
    "  \"we'd\": \"we had\",\n",
    "  \"we'd've\": \"we would have\",\n",
    "  \"we'll\": \"we will\",\n",
    "  \"we'll've\": \"we will have\",\n",
    "  \"we're\": \"we are\",\n",
    "  \"we've\": \"we have\",\n",
    "  \"weren't\": \"were not\",\n",
    "  \"what'll\": \"what will\",\n",
    "  \"what'll've\": \"what will have\",\n",
    "  \"what're\": \"what are\",\n",
    "  \"what's\": \"what is\",\n",
    "  \"what've\": \"what have\",\n",
    "  \"when's\": \"when is\",\n",
    "  \"when've\": \"when have\",\n",
    "  \"where'd\": \"where did\",\n",
    "  \"where's\": \"where is\",\n",
    "  \"where've\": \"where have\",\n",
    "  \"who'll\": \"who will\",\n",
    "  \"who'll've\": \"who will have\",\n",
    "  \"who's\": \"who is\",\n",
    "  \"who've\": \"who have\",\n",
    "  \"why's\": \"why is\",\n",
    "  \"why've\": \"why have\",\n",
    "  \"will've\": \"will have\",\n",
    "  \"won't\": \"will not\",\n",
    "  \"won't've\": \"will not have\",\n",
    "  \"would've\": \"would have\",\n",
    "  \"wouldn't\": \"would not\",\n",
    "  \"wouldn't've\": \"would not have\",\n",
    "  \"ya'll\": \"you all\",\n",
    "  \"y'all\": \"you all\",\n",
    "  \"y'alls\": \"you alls\",\n",
    "  \"y'all'd\": \"you all would\",\n",
    "  \"y'all'd've\": \"you all would have\",\n",
    "  \"y'all're\": \"you all are\",\n",
    "  \"y'all've\": \"you all have\",\n",
    "  \"yday\": \"yesterday\",\n",
    "  \"you'd\": \"you had\",\n",
    "  \"you'd've\": \"you would have\",\n",
    "  \"you'll\": \"you will\",\n",
    "  \"you'll've\": \"you will have\",\n",
    "  \"you're\": \"you are\",\n",
    "  \"you've\": \"you have\"\n",
    "}\n",
    "\"\"\"\n",
    "    create a regular expression pattern using 'contractions_dict'\n",
    "    this pattern is used to identify contractions in the given input string\n",
    "\"\"\"\n",
    "contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "\n",
    "\"\"\"\n",
    "    replaces all occurrences of contractions in the input string with their expanded forms\n",
    "    'replace' function is used as the replacement function, and it's applied to each match found by the regular expression\n",
    "\"\"\"\n",
    "def expand_contractions(tweet, contractions_dict=contractions_dict):\n",
    "    # takes a 'match' object and returns corresponded expansion form\n",
    "    tweet = tweet.replace(\"â€™\", \"'\")\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "    return contractions_re.sub(replace, tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3d18642f-4073-48fa-82d3-b5fa719a3dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweet = \"Wow, my dad yday: â€œyou donâ€™t take those stupid depression drugs anymore though, do you? Because theyâ€™re the absolute worst thing [and there is never a need for them]!â€  Ainâ€™t it great when your own family is so supportive? My momâ€™s and sisterâ€™s stance on this is similar, btw...\"\n",
    "#expand_contractions(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "029963b7-63d5-4346-ab95-3203f893c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    convert accent characters into standard ASCII characters\n",
    "    Such as: rÃ©sumÃ©, cafÃ©, prÃ³test, divorcÃ© => resume, cafe, protest, divorce\n",
    "\"\"\"\n",
    "def convert_accented_chars(tweet):\n",
    "    return unicodedata.normalize('NFKD', tweet).encode('ascii', 'ignore').decode('utf-8', 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "43ebdce0-ba61-4c98-a2b3-1d607e43cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(tweet):\n",
    "    return ''.join(char for char in tweet if not emoji.is_emoji(char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a6c88661-7cba-4339-bc2e-296bdd2776c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove mentions (and tags ?)\n",
    "def remove_mentions(tweet):\n",
    "    return re.sub(r'@\\S*', '', tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "99f795c2-f59e-47be-8850-bfa506569b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove digits\n",
    "def remove_digits(tweet):\n",
    "    return ''.join(char for char in tweet if not char.isdigit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1824d57e-4806-481d-b806-ab146e9d55dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(tweet):\n",
    "    pattern = r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s]' \n",
    "    return re.sub(pattern, '', tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "dc087b83-14b6-4ff9-9b9c-7607cf60d994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(tweet):\n",
    "    return ''.join([char for char in tweet if char not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a7f16b72-3353-42ea-9c95-d9e208f70743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tweet):\n",
    "    word_tokens = nltk.word_tokenize(tweet) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stopword_list]\n",
    "    tweet = ' '.join(filtered_sentence)\n",
    "    return tweet\n",
    "\n",
    "#remove_stopwords('wow my dad yday you do not take those stupi would depression drugs anymore though do you because they are the absolute worst thing and there is never a need for them  am not it great when your own family is so supporti have my moms and sisters stance on this is similar by the way')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "10c4da8a-fdef-4190-9f38-5bf05c0989ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['tweet'] = df_clean['tweet'].apply(to_lowercase)\n",
    "df_clean['tweet'] = df_clean['tweet'].apply(remove_url)\n",
    "df_clean['tweet'] = df_clean['tweet'].apply(expand_contractions)\n",
    "df_clean['tweet'] = df_clean['tweet'].apply(convert_accented_chars)\n",
    "df_clean['tweet'] = df_clean['tweet'].apply(remove_emojis)\n",
    "df_clean['tweet'] = df_clean['tweet'].apply(remove_mentions)\n",
    "df_clean['tweet'] = df_clean['tweet'].apply(remove_digits)\n",
    "df_clean['tweet'] = df_clean['tweet'].apply(remove_special_characters)\n",
    "df_clean['tweet'] = df_clean['tweet'].apply(remove_punctuation)\n",
    "df_clean['tweet'] = df_clean['tweet'].apply(remove_stopwords)\n",
    "\n",
    "#df_clean.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8bbefda7-1f68-4594-bf0c-bb67f880cbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22963 entries, 0 to 22962\n",
      "Data columns (total 8 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   vader_sentiment_label  22963 non-null  int64  \n",
      " 1   vader_score            22963 non-null  float64\n",
      " 2   tweet                  22963 non-null  object \n",
      " 3   tweet_length           22963 non-null  int64  \n",
      " 4   url_link               22963 non-null  int64  \n",
      " 5   pos_emoji              22963 non-null  int64  \n",
      " 6   neg_emoji              22963 non-null  int64  \n",
      " 7   profanity_word         22963 non-null  int64  \n",
      "dtypes: float64(1), int64(6), object(1)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9790f405-6a6d-49cc-b21f-96790af3aba3",
   "metadata": {},
   "source": [
    "### Some tweets remained as an empty string after unnecessary parts were removed.\n",
    "Such as:\n",
    "    \"ðŸ’œðŸ’œðŸ’œ @NewYorkTimes\" => \" \"\n",
    "\n",
    "These rows must be removed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5073aa68-a59e-481c-ba2d-41694a2b3dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vader_sentiment_label</th>\n",
       "      <th>vader_score</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_length</th>\n",
       "      <th>url_link</th>\n",
       "      <th>pos_emoji</th>\n",
       "      <th>neg_emoji</th>\n",
       "      <th>profanity_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>1</td>\n",
       "      <td>0.7845</td>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723</th>\n",
       "      <td>1</td>\n",
       "      <td>0.6369</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td></td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22650</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22708</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22731</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22779</th>\n",
       "      <td>1</td>\n",
       "      <td>0.4939</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22820</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9274</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>133 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       vader_sentiment_label  vader_score tweet  tweet_length  url_link  \\\n",
       "1281                       0       0.0000                   2         0   \n",
       "1447                       1       0.7845                   7         0   \n",
       "1450                       0       0.0000                   0         0   \n",
       "1723                       1       0.6369                   3         0   \n",
       "1732                       0       0.0000                 132         0   \n",
       "...                      ...          ...   ...           ...       ...   \n",
       "22650                      0       0.0000                   0         0   \n",
       "22708                      0       0.0000                   0         0   \n",
       "22731                      0       0.0000                   0         0   \n",
       "22779                      1       0.4939                   2         0   \n",
       "22820                      1       0.9274                   3         0   \n",
       "\n",
       "       pos_emoji  neg_emoji  profanity_word  \n",
       "1281           0          0               0  \n",
       "1447           0          0               0  \n",
       "1450           0          0               0  \n",
       "1723           0          0               0  \n",
       "1732           0          0               0  \n",
       "...          ...        ...             ...  \n",
       "22650          0          0               0  \n",
       "22708          0          0               0  \n",
       "22731          0          0               0  \n",
       "22779          0          0               0  \n",
       "22820          0          0               0  \n",
       "\n",
       "[133 rows x 8 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean[df_clean['tweet'].str.len() == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "cd7e9530-1320-434a-80e8-744b458a8093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22830"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove rows where 'tweet' column is empty\n",
    "df_clean = df_clean[df_clean['tweet'].str.len() > 0]\n",
    "\n",
    "len(df_clean.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705e2ecc-89cd-4504-911d-9d5aba117661",
   "metadata": {},
   "source": [
    "After cleaning the text, the text should be processed with Stemmer before tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b47c2e56-e2f4-4bea-abb9-08034e735f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def get_stem(tweet, stemmer=stemmer):\n",
    "    return ' '.join([stemmer.stem(word) for word in tweet.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5f273a3a-0b9f-4c71-a45b-05cd92a33700",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['tweet'] = df_clean['tweet'].apply(get_stem) \n",
    "#df_clean.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b06cb7-bac7-435e-baa2-64097f77b8a9",
   "metadata": {},
   "source": [
    "## Tokenization and Embedding Extraction with BERT\n",
    "The text is cleaned and should be tokenized in a way thay the embeddings can be extracted for later training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ef16e31f-d9bb-41f1-b076-f152e691ddf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vader_sentiment_label</th>\n",
       "      <th>vader_score</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_length</th>\n",
       "      <th>url_link</th>\n",
       "      <th>pos_emoji</th>\n",
       "      <th>neg_emoji</th>\n",
       "      <th>profanity_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.2699</td>\n",
       "      <td>wow dad yesterday take stupi would depress dru...</td>\n",
       "      <td>278</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.5995</td>\n",
       "      <td>part realli harmfult lot peopl went everi gui ...</td>\n",
       "      <td>274</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.3382</td>\n",
       "      <td>one way got depress learn danc rain sourc stre...</td>\n",
       "      <td>208</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.8643</td>\n",
       "      <td>see wan na one say ptsd depress andor anxieti ...</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.8316</td>\n",
       "      <td>clinic depress palpabl hopeless gener</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   vader_sentiment_label  vader_score  \\\n",
       "0                      0      -0.2699   \n",
       "1                      0      -0.5995   \n",
       "2                      1       0.3382   \n",
       "3                      0      -0.8643   \n",
       "4                      0      -0.8316   \n",
       "\n",
       "                                               tweet  tweet_length  url_link  \\\n",
       "0  wow dad yesterday take stupi would depress dru...           278         0   \n",
       "1  part realli harmfult lot peopl went everi gui ...           274         0   \n",
       "2  one way got depress learn danc rain sourc stre...           208         0   \n",
       "3  see wan na one say ptsd depress andor anxieti ...           114         0   \n",
       "4              clinic depress palpabl hopeless gener            78         0   \n",
       "\n",
       "   pos_emoji  neg_emoji  profanity_word  \n",
       "0          0          0               0  \n",
       "1          0          0               0  \n",
       "2          0          0               0  \n",
       "3          0          0               0  \n",
       "4          0          0               0  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bert = df_clean.copy()\n",
    "df_bert.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c84d89-6280-46ea-b681-b75af462d66f",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1e79b33e-f26a-41fc-9d61-77e508e6ed71",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Returns tokens of the tweet, and tensors of the tokens and segment ids\n",
    "def get_tokenization(tweet, tokenizer=tokenizer):\n",
    "    marked_text = \"[CLS] \" + tweet + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "    # Map the token strings to their vocabulary indeces.\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    # segments_ids is used as an indicator of multiple sentences.\n",
    "    # We're taking only one sentence here, so segments_ids will be filled up with only '1's\n",
    "    # Mark each of the tokens as belonging to sentence \"1\"\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensor = torch.tensor([segments_ids])\n",
    "\n",
    "    return tokenized_text, tokens_tensor, segments_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e427c99d-464e-445c-96ac-977345e74008",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cacd8f-5d50-4f65-a479-8159e097d046",
   "metadata": {},
   "source": [
    "### Extracting Embeddings - Sentence Vectors\n",
    "\n",
    "Understanding what `extract_sentence_embeddings` does:\n",
    "- It predict hidden states features for each layer.\n",
    "- `hidden_states` object has four dimensions, in the following order:\n",
    "- 1) The layer number (13 = 12 + input embeddings)\n",
    "  2) The batch number (1 sentence)\n",
    "  3) The word/token number (depends on the given `tokens_tensor`)\n",
    "  4) The hidden unit/feature number (768 features)\n",
    "- Then, average the second to last hiden layer of each token producing a single 768 length vector to get a single vector for the entire sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e0e06266-1959-43b4-a854-f140f6f5085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Side note: torch.no_grad tells PyTorch not to construct the compute graph \n",
    "    during this forward pass (since we wonâ€™t be running backprop here).\n",
    "    This just reduces memory consumption and speeds things up a little.\n",
    "\"\"\"\n",
    "\n",
    "def extract_sentence_embeddings(tokens_tensor, segments_tensor, model=model):\n",
    "    # Run the text through BERT, and collect all of the hidden states produced\n",
    "    # from all 12 layers. \n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensor)\n",
    "       \n",
    "        # `hidden_states` has shape [13 x 1 x 22 x 768]\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "        # `token_vecs` is a tensor with shape [22 x 768]\n",
    "        token_vecs = hidden_states[-2][0]\n",
    "\n",
    "        # Calculate the average of all 22 token vectors.\n",
    "        sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "        \n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ea58ae-4a64-4cbb-b250-b029179acfcc",
   "metadata": {},
   "source": [
    "Let's test the tokenization and extraction of sentence embeddings with the first tweet in our cleaned data set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "cb04fc97-4877-47b6-851f-35fcfdc58bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wow dad yesterday take stupi would depress drug anymor though absolut worst thing never need great famili supporti mom sister stanc similar way'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = df_bert['tweet'][0]\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4f7ba66c-1f2f-478e-9baa-48656b11a09e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[CLS]',\n",
       "  'wow',\n",
       "  'dad',\n",
       "  'yesterday',\n",
       "  'take',\n",
       "  'stu',\n",
       "  '##pi',\n",
       "  'would',\n",
       "  'de',\n",
       "  '##press',\n",
       "  'drug',\n",
       "  'any',\n",
       "  '##mo',\n",
       "  '##r',\n",
       "  'though',\n",
       "  'abs',\n",
       "  '##ol',\n",
       "  '##ut',\n",
       "  'worst',\n",
       "  'thing',\n",
       "  'never',\n",
       "  'need',\n",
       "  'great',\n",
       "  'fa',\n",
       "  '##mi',\n",
       "  '##li',\n",
       "  'support',\n",
       "  '##i',\n",
       "  'mom',\n",
       "  'sister',\n",
       "  'stan',\n",
       "  '##c',\n",
       "  'similar',\n",
       "  'way',\n",
       "  '[SEP]'],\n",
       " tensor([[  101, 10166,  3611,  7483,  2202, 24646,  8197,  2052,  2139, 20110,\n",
       "           4319,  2151,  5302,  2099,  2295, 14689,  4747,  4904,  5409,  2518,\n",
       "           2196,  2342,  2307,  6904,  4328,  3669,  2490,  2072,  3566,  2905,\n",
       "           9761,  2278,  2714,  2126,   102]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text, tokens_tensor, segments_tensor = get_tokenization(tweet)\n",
    "tokenized_text, tokens_tensor, segments_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b72af2cc-97c4-4417-aa06-3a829f7e1654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-7.8889e-01,  3.2795e-01,  9.7737e-01, -3.5318e-01,  2.9939e-01,\n",
       "        -4.3548e-01,  6.1954e-01,  6.1555e-01, -2.1930e-01, -6.4830e-01,\n",
       "         6.3507e-01, -5.2176e-01,  7.4387e-01,  8.0955e-01, -1.4502e+00,\n",
       "         3.5829e-01,  5.1148e-01,  6.6309e-01, -1.0264e-01, -2.3209e-01,\n",
       "         8.7779e-01, -3.3429e-01, -1.0678e-01,  1.6091e-01,  3.2531e-02,\n",
       "        -4.5209e-01,  3.9826e-01,  4.1566e-02, -6.6612e-03,  1.5509e-01,\n",
       "         3.1926e-01,  2.1097e-01, -3.9433e-01, -3.6388e-01,  8.4692e-02,\n",
       "        -2.0377e-01,  1.4267e-01,  9.0458e-01, -1.5283e-01,  2.7483e-01,\n",
       "         1.7792e-01, -6.7930e-01, -4.8960e-02,  1.0906e-01, -6.2476e-01,\n",
       "        -5.9140e-02, -1.2102e-01, -2.2941e-01,  4.8750e-02,  1.4828e-01,\n",
       "        -1.9536e-01, -5.0544e-01, -6.5010e-01, -4.8235e-01, -8.1248e-02,\n",
       "         4.1584e-01,  2.3588e-01, -4.8958e-01,  1.2158e-01, -2.8020e-01,\n",
       "         4.9251e-02,  1.1793e-01,  3.6503e-01, -2.1966e-01, -2.4879e-02,\n",
       "        -2.4909e-01, -1.1744e-01,  1.4650e-01, -4.3091e-01, -3.6206e-01,\n",
       "        -4.5388e-01,  4.7251e-01, -2.5114e-01,  6.6191e-01, -3.3303e-01,\n",
       "         1.7280e-01, -6.3091e-01, -6.5320e-02, -2.1996e-01, -2.2468e-01,\n",
       "        -4.2614e-01, -9.5005e-02, -1.1102e+00,  2.8703e-01,  2.3659e-01,\n",
       "        -2.7190e-01,  7.1969e-02,  8.7080e-02, -7.1304e-01,  7.5273e-01,\n",
       "         9.4521e-01,  4.1648e-01, -1.6380e-01,  1.4334e-01,  3.4777e-01,\n",
       "        -1.2139e-01, -3.3318e-01, -4.6492e-01,  3.4175e-01,  2.4762e-01,\n",
       "        -2.5479e-01, -2.1398e-01, -1.1643e-01, -3.5549e-01,  8.5122e-02,\n",
       "         2.1009e-01, -6.7153e-01,  5.2426e-01,  7.5938e-01,  1.0362e-01,\n",
       "         6.0281e-02,  6.8540e-01,  1.0422e-01, -5.6827e-02, -8.3923e-01,\n",
       "        -5.7530e-01,  2.2762e-01,  1.1679e-01,  2.8943e-01, -1.8456e-01,\n",
       "        -7.7860e-01, -7.0432e-03,  4.3137e-01,  4.1960e-01,  4.2104e-01,\n",
       "         3.3289e-01, -4.1817e-01, -6.9694e-01,  1.8840e-01,  7.0889e-01,\n",
       "        -7.0545e-02,  3.3854e-01, -6.3672e-01, -1.1240e-01,  8.3423e-02,\n",
       "        -4.2840e-02, -2.8462e-02, -1.7225e-01, -6.2121e-01, -3.0105e-01,\n",
       "        -3.7167e-01,  1.0053e-01, -3.6248e-02,  3.9497e-01, -1.2542e-01,\n",
       "        -5.6380e-01, -4.4013e-01, -3.7768e-02, -8.2934e-02,  8.6708e-01,\n",
       "        -8.2841e-01, -4.1718e-01, -4.0803e-01,  1.9065e-01, -8.0113e-01,\n",
       "        -1.1098e-01, -2.5035e-01,  4.1857e-01,  1.9657e-01,  3.4684e-01,\n",
       "        -1.4145e-01, -3.7528e-01, -5.0277e-02, -3.9137e-01,  1.5450e-01,\n",
       "        -7.6681e-01,  8.8967e-02,  2.1531e-01, -4.2249e-01,  1.4239e-02,\n",
       "         2.0858e-01,  3.6750e-01,  1.0405e-01, -2.1338e-01,  3.8288e-01,\n",
       "        -6.7364e-01,  2.3666e-01, -1.4468e-01, -1.6858e-01,  8.4713e-01,\n",
       "        -3.7967e-01,  9.3295e-01, -5.0639e-01,  7.0203e-01,  3.5266e-01,\n",
       "         7.2865e-01,  3.9610e-01,  3.1260e-01,  8.7897e-01, -5.4822e-01,\n",
       "        -5.1121e-01, -1.4515e-02,  6.0155e-02, -1.8916e-01,  6.3592e-01,\n",
       "        -4.6051e-01, -1.0980e-01,  1.9614e-02,  1.2724e-01,  2.6803e-01,\n",
       "         1.5859e-01,  2.7937e-01,  1.1534e-01, -1.3950e-01, -4.4152e-01,\n",
       "         5.0579e-01, -2.7904e-01,  3.0133e-01,  1.1568e-01,  4.2723e-01,\n",
       "         3.7605e-01,  6.5060e-02,  2.5548e-01,  2.5520e-01,  3.4916e-01,\n",
       "        -5.5385e-01, -2.7441e-01,  3.6110e-01,  2.7888e-01,  4.5620e-01,\n",
       "        -9.9711e-02, -3.6508e-01, -4.6295e-01,  3.9012e-01, -4.2423e-01,\n",
       "         7.4773e-01, -6.0220e-02, -3.8199e-02, -2.3384e-01, -3.0178e-02,\n",
       "        -1.5188e-01, -1.8200e-01,  9.8813e-01, -4.6512e-01,  1.8557e-01,\n",
       "         1.8431e-01,  5.6387e-01,  5.2414e-01, -1.4805e-01, -3.5643e-01,\n",
       "        -8.6794e-01,  8.1570e-02,  3.6234e-01, -2.6093e-02, -6.1096e-01,\n",
       "        -1.6846e-01, -6.8868e-01, -8.9014e-01, -1.0214e-01, -6.7067e-01,\n",
       "        -2.4509e-01,  2.0451e-01,  4.5730e-01, -4.5633e-01, -7.2640e-02,\n",
       "        -1.5487e-02,  5.7756e-01, -5.1503e-01, -8.8757e-01, -7.3534e-01,\n",
       "        -2.5001e-01,  2.7714e-01,  1.0729e-01,  5.0931e-03, -5.8301e-01,\n",
       "         2.0274e-02, -2.6730e-01,  5.8129e-01,  2.0939e-01, -5.4320e-01,\n",
       "         5.4283e-01, -1.9848e-01,  2.0970e-01,  8.0974e-01,  6.8101e-01,\n",
       "         6.5808e-01, -2.9284e-03,  4.8675e-01,  1.3297e-01, -7.0890e-01,\n",
       "         7.6965e-01,  9.7005e-01,  6.3645e-01, -9.2445e-02, -1.8382e-03,\n",
       "        -4.2030e-02, -8.4504e-01, -8.8024e-03,  1.3550e-01, -8.9943e-01,\n",
       "         3.1116e-01,  1.5579e-01,  2.9503e-01,  1.8865e-01, -1.5141e-01,\n",
       "         5.9821e-01, -4.7511e-01,  9.2565e-02, -3.4020e-01, -8.9072e-01,\n",
       "        -8.9327e-01,  7.4037e-01,  3.1325e-02,  1.1097e+00,  1.9451e-01,\n",
       "        -6.3827e-01, -6.9194e-01,  4.1911e-01, -1.3618e+01, -4.5515e-01,\n",
       "        -3.6983e-01, -1.1071e-01,  9.1826e-01, -1.2603e-01,  5.1358e-01,\n",
       "        -3.5503e-01,  1.2045e-01,  4.5983e-01,  4.8728e-02, -3.1914e-01,\n",
       "         4.4610e-01,  8.1058e-01,  8.4559e-02,  2.4942e-01, -2.2343e-01,\n",
       "         2.5186e-01, -9.5718e-01,  4.3575e-01,  2.8836e-01, -1.1680e-01,\n",
       "         8.0151e-01,  1.7367e-02,  4.9727e-01, -6.2659e-01, -3.9899e-01,\n",
       "         5.4073e-01, -5.3321e-01, -2.8552e-01,  6.8595e-01, -1.4812e-01,\n",
       "        -3.7143e-01,  2.3387e-01, -1.2773e-01,  1.8741e-01,  4.2480e-01,\n",
       "        -6.5984e-01,  6.7725e-01, -4.6270e-01, -3.1335e-01, -7.4028e-01,\n",
       "        -4.1974e-01,  4.5719e-01, -3.5183e-01,  3.2458e-02, -1.0449e-01,\n",
       "         2.9539e-01,  2.1684e-02,  3.2693e-01, -4.3981e-01, -4.6204e-01,\n",
       "        -6.2415e-02, -5.1104e-01, -3.0986e-01, -2.2756e-01,  6.8366e-01,\n",
       "         1.2234e+00, -5.7709e-01, -9.0945e-01, -6.1806e-01,  8.4104e-01,\n",
       "        -1.1060e+00, -6.2694e-01,  7.2711e-01,  8.0862e-01, -3.5378e-01,\n",
       "        -2.1261e-01,  2.3549e-01,  4.1340e-01,  1.1054e-01, -4.3340e-01,\n",
       "         2.7336e-01, -8.3322e-01, -7.3415e-02,  8.4104e-01, -6.0726e-02,\n",
       "         6.6727e-01, -4.5742e-01,  3.3056e-01,  6.9844e-02,  2.0623e-01,\n",
       "         6.3409e-03, -4.7718e-01, -5.9035e-01, -1.1965e-01,  1.6684e-01,\n",
       "        -1.4116e+00, -7.8115e-01, -4.8233e-01,  4.2823e-01, -4.0874e-01,\n",
       "         2.4062e-01,  4.0652e-01,  2.7856e-01, -2.9496e-01,  1.1390e-01,\n",
       "        -4.2736e-01,  7.4024e-01, -4.8011e-01,  6.0368e-01,  6.8131e-01,\n",
       "        -2.3445e-01, -3.0377e-01, -1.7504e-01, -8.1379e-02,  2.7914e-01,\n",
       "        -4.7378e-01,  7.0073e-02,  6.1369e-02,  1.6542e-01, -2.8950e-01,\n",
       "         5.2383e-01,  3.1517e-02, -1.2135e-01, -5.9667e-01,  1.0674e+00,\n",
       "         2.2941e-02, -2.0552e-01, -7.9076e-02,  2.1264e-01, -7.6877e-02,\n",
       "        -1.1174e+00, -1.4831e+00, -3.6893e-01,  8.7097e-02, -6.8439e-01,\n",
       "         9.2909e-01, -6.9482e-01,  7.6778e-01, -6.1305e-01, -4.3448e-01,\n",
       "        -1.5785e-01,  6.0985e-01,  3.4854e-01, -6.1835e-01, -3.0323e-01,\n",
       "        -8.2151e-01,  2.9161e-01, -2.5209e-01,  3.5079e-01, -4.9047e-01,\n",
       "         2.7446e-01, -1.9699e-03,  1.7892e-01,  3.6442e-01, -2.1012e-01,\n",
       "        -3.7509e-01, -5.2395e-01,  1.7307e-01, -5.9111e-01, -1.0947e+00,\n",
       "         7.6945e-01,  1.7127e-01, -6.8429e-02,  1.9590e-01,  1.2525e-01,\n",
       "        -3.1110e-01, -2.2126e-01, -8.3520e-01, -6.0963e-01,  1.7635e-02,\n",
       "        -1.9480e-01,  9.6420e-02, -3.1510e-01,  3.8831e-01, -2.3991e-01,\n",
       "        -4.8739e-01,  5.0825e-01,  3.4990e-01, -4.5184e-01,  2.6773e-01,\n",
       "        -8.3240e-01,  4.4406e-02, -5.2591e-01,  5.6829e-01, -9.5102e-02,\n",
       "        -3.3313e-02, -1.4115e-01,  4.1822e-01,  1.3999e-02,  3.0549e-02,\n",
       "         1.0636e-01,  2.8063e-01,  9.1407e-01,  6.1520e-02,  5.3053e-01,\n",
       "        -5.5158e-01,  6.1648e-01,  7.7146e-01, -8.1503e-01,  1.0698e+00,\n",
       "        -4.8212e-01, -2.3690e-01,  6.6160e-01, -1.3026e-02,  6.1329e-01,\n",
       "         4.1895e-02,  4.4711e-01,  8.8583e-01,  4.1035e-01,  5.2067e-01,\n",
       "         8.1889e-01,  1.5891e-02, -2.9225e-01, -5.3825e-01, -4.0996e-01,\n",
       "         3.9069e-01,  4.9149e-01,  1.2917e-01,  2.6653e-01, -5.8091e-01,\n",
       "         3.5415e-01,  3.6947e-01,  3.1275e-01,  1.6385e-01,  1.8666e-01,\n",
       "        -3.2398e-01, -1.0778e+00,  1.6271e-01,  1.8080e-01, -7.7763e-01,\n",
       "        -7.0175e-01,  1.1219e-01, -7.0881e-01, -3.9974e-01,  8.1342e-01,\n",
       "         3.4469e-02,  2.6307e-01,  8.3829e-02,  1.1233e-01, -1.0406e+00,\n",
       "         5.3075e-01,  2.7801e-01,  3.6656e-01, -2.6206e-01,  4.9173e-01,\n",
       "         8.6950e-02,  6.4307e-01, -2.9035e-01, -2.9962e-01, -9.2344e-02,\n",
       "        -2.8727e-01, -1.0359e+00,  3.9179e-02,  1.0128e+00,  3.0838e-01,\n",
       "         1.4140e-01,  1.7374e-01,  5.9931e-01,  5.9035e-01, -4.0409e-01,\n",
       "        -3.9819e-01, -1.7420e-01, -1.8318e-01, -9.1666e-01, -8.8617e-01,\n",
       "        -2.7549e-01, -5.1745e-01,  7.3509e-01, -1.0735e+00,  2.8653e-01,\n",
       "        -5.3520e-01, -7.8059e-01, -1.8893e-01,  3.4733e-01,  8.2275e-01,\n",
       "         4.8079e-01,  1.2413e-01, -6.4959e-01,  2.8634e-01, -5.6943e-01,\n",
       "         2.7487e-01, -2.2823e-01, -3.1632e-01, -2.8784e-01, -6.5086e-01,\n",
       "         1.2392e-02,  9.9932e-01,  2.6280e-01, -4.4848e-01, -4.5345e-01,\n",
       "        -3.8291e-01, -5.0009e-04, -1.6882e-01,  2.9749e-02,  8.3143e-02,\n",
       "        -7.2790e-01, -6.1033e-01,  5.2311e-01,  2.7967e-01,  1.1320e-01,\n",
       "        -5.5523e-01, -1.0236e+00,  4.7243e-01,  6.0585e-01, -1.6975e-01,\n",
       "        -5.0899e-01,  4.6812e-01,  7.7076e-01, -3.5975e-01, -3.5304e-01,\n",
       "         1.0808e+00, -6.0495e-01,  1.3167e-01,  5.5571e-01, -4.1155e-01,\n",
       "         4.0837e-01,  3.0041e-01,  6.8681e-01, -6.4017e-01, -4.0875e-02,\n",
       "         1.0717e+00,  9.3707e-02,  1.4590e-01, -1.1098e+00, -2.1490e-01,\n",
       "        -2.6189e-01,  3.1478e-01,  5.1440e-01, -3.4070e-01, -4.1877e-04,\n",
       "         5.9434e-01, -6.4556e-01, -1.7648e-01,  2.0352e-02, -3.3030e-01,\n",
       "         2.6625e-01, -8.6598e-02,  1.2120e-01,  3.8988e-01, -3.9031e-01,\n",
       "        -3.4677e-01, -7.3293e-02,  5.4059e-01, -1.8198e-01, -1.7375e-01,\n",
       "         1.1651e-01, -8.3626e-02,  1.3413e-01,  7.0716e-01,  3.8930e-01,\n",
       "        -1.4495e-01,  3.4241e-01, -4.3356e-02, -4.9578e-01, -2.8943e-01,\n",
       "         1.5119e-01,  9.2322e-01,  9.2017e-01,  1.5921e-01, -5.0562e-02,\n",
       "         3.4540e-01, -1.6984e-01,  5.1143e-01,  1.2645e-01,  1.5387e-01,\n",
       "         3.7091e-01, -3.9303e-01,  3.1637e-01,  3.3676e-01, -2.1355e-01,\n",
       "        -2.8433e-02,  2.4086e-01,  5.1403e-01,  3.7691e-01, -2.6777e-01,\n",
       "         1.0561e-01, -6.6885e-01, -9.1645e-01,  2.7053e-01, -7.7712e-01,\n",
       "        -4.3251e-01,  7.2626e-01, -1.1166e-01, -2.8911e-01, -3.1938e-01,\n",
       "        -1.0203e-01, -7.8053e-01,  1.5018e-01, -5.5048e-01,  1.0625e-01,\n",
       "        -4.6571e-01, -5.5186e-01,  2.4571e-01, -3.7157e-01, -5.3917e-01,\n",
       "        -6.3130e-01, -2.6282e-01,  8.6228e-02, -3.4916e-01,  7.0725e-01,\n",
       "        -6.3000e-01, -1.7798e-01,  5.5295e-02,  4.2602e-01,  3.4665e-01,\n",
       "        -1.9018e-01,  7.5598e-01,  3.0515e-01,  1.8314e-01,  1.9986e-01,\n",
       "         1.7080e-01, -4.6369e-01,  2.3566e-01, -3.9446e-01, -3.3313e-01,\n",
       "        -1.6591e-01, -4.0077e-01,  1.1497e-02, -8.8692e-01,  9.4888e-01,\n",
       "        -8.9217e-02,  3.8126e-01, -3.8906e-02,  3.3357e-02, -7.0699e-02,\n",
       "        -4.2402e-01, -3.6403e-01, -3.4345e-01, -6.7057e-01, -3.3194e-01,\n",
       "        -1.7268e-01, -3.9048e-01, -6.0761e-01, -7.5729e-01, -5.3069e-01,\n",
       "         2.5437e-01, -9.0913e-02, -2.0090e-01, -3.3272e-01,  5.7074e-01,\n",
       "         7.8520e-01, -5.1963e-01, -1.3198e-01,  2.7695e-01, -7.1834e-01,\n",
       "        -9.5132e-03,  1.1114e-01,  5.8394e-01,  3.7581e-01, -5.6798e-01,\n",
       "        -4.1467e-01,  2.9295e-01,  1.1538e+00, -5.4748e-03,  4.3549e-01,\n",
       "         7.0207e-01, -2.3263e-01, -1.3651e+00, -6.5821e-01,  2.1918e-02,\n",
       "         1.7559e-01,  5.2379e-01, -1.3766e-01,  3.0547e-02, -3.2284e-01,\n",
       "        -1.5379e+00, -8.9105e-02, -1.6422e-01])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embedding = extract_sentence_embeddings(tokens_tensor, segments_tensor)\n",
    "sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "096f8849-de96-4d6d-a1cc-3dc24e65b250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a146d907-fbfd-45c4-8039-0f20cf08c591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: At this point, I have to get the vectors and relevant attributes per tweet.\n",
    "# I will probably use a tuple for this:\n",
    "\n",
    "# => [(tensor, tweet_length, url_link , pos_emoji , neg_emoji, profanity_word, class)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c913be4-06be-4120-8e31-d7d2329164f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
